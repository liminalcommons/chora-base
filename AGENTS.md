# AGENTS.md - chora-base Template Repository

**Purpose**: Machine-readable instructions for AI agents working on the chora-base template repository.

**Last Updated**: 2025-11-08 (Windows compatibility + cross-platform enforcement)

---

## ğŸ”´ CROSS-PLATFORM REMINDER

**ALL code MUST work on Windows, Mac, and Linux without modification.**

Before writing Python scripts, read: **[scripts/AGENTS.md](scripts/AGENTS.md)** for cross-platform patterns.

**Quick Template**: Copy [templates/cross-platform/python-script-template.py](templates/cross-platform/python-script-template.py)

**Validation**: `python scripts/validate-windows-compat.py --file your-script.py`

---

## âš ï¸ CRITICAL: chora-base is a TEMPLATE SOURCE

**DO NOT** try to "set up chora-base" as if it were a project to develop.

**chora-base** is a **template repository** used to **generate other projects**.

### Decision Tree for Agents

**Are you trying to CREATE A NEW PROJECT using chora-base?**

â†’ **YES**: Use the fast-setup script:

```bash
python scripts/create-model-mcp-server.py \
    --name "Your Project Name" \
    --namespace yournamespace \
    --output ~/projects/your-project
```

See: [README.md](README.md) for complete instructions, or [docs/user-docs/quickstart-mcp-server.md](docs/user-docs/quickstart-mcp-server.md)

**Time**: 5-10 minutes to fully-configured project

---

**Are you DEVELOPING chora-base itself** (contributing to the template)?

â†’ **YES**: Continue reading this file

**What you're working on**: The template source code, SAP definitions, scripts, documentation

**Key directories**:
- `static-template/` - Files that get copied to generated projects
- `docs/skilled-awareness/` - SAP documentation (30+ capabilities)
- `scripts/` - Automation scripts (create-model-mcp-server.py, install-sap.py, etc.)

---

## Project Overview

**chora-base** is a blueprint-driven Python project template designed for LLM-intelligent development. It generates production-ready Python projects with built-in support for AI coding agents, comprehensive documentation, and quality gates without relying on Copier.

**Repository Type**: Template repository (generates other projects)
**Primary Users**: Human developers and AI agents generating/maintaining Python projects
**Key Technology Stack**: Static scaffolding (`static-template/`), and Skilled Awareness Packages (SAP) for capability governance

### Key Concepts

- **Template vs Generated Project**: chora-base is the template; generated projects are adopters
- **Blueprint Generation**: `.blueprint` files define how adopters bootstrap projects via scripted or agent-led workflows
- **Static Assets**: `static-template/` contains ready-to-copy files used by blueprints
- **Skilled Awareness Packages (SAPs)**: Each major capability ships with charter, protocol, awareness guide, adoption blueprint, and ledger entry to guide humans and AI agents
- **Upgrade Path**: Adopters follow SAP adoption blueprints and ledger/broadcast updates to stay aligned with the template

---

## Skilled Awareness Packages (SAPs)

### Overview

chora-base packages all major capabilities as **Skilled Awareness Packages (SAPs)** â€” complete, installable bundles with clear contracts and agent-executable blueprints.

**Why SAPs Matter**:
- Clear contracts (explicit guarantees, no assumptions)
- Predictable upgrades (sequential adoption, migration blueprints)
- Machine-readable (AI agents can parse and execute)
- Governance (versioning, change management, tracking)

### SAP Structure

Every SAP includes:

1. **5 Core Artifacts**:
   - Capability Charter (problem, scope, outcomes)
   - Protocol Specification (technical contract)
   - Awareness Guide (agent execution patterns)
   - Adoption Blueprint (installation steps)
   - Traceability Ledger (adopter tracking)

2. **Infrastructure** (schemas, templates, configs, directories)

3. **Testing Layer** (optional validation)

### Key Documents

**Root Protocol**: [SKILLED_AWARENESS_PACKAGE_PROTOCOL.md](SKILLED_AWARENESS_PACKAGE_PROTOCOL.md)
- Defines what SAPs are and how they work
- Installation pattern (blueprint-based, not scripts)
- Integration with DDD â†’ BDD â†’ TDD
- Scope levels (Vision & Strategy, Planning, Implementation)

**SAP Index**: [docs/skilled-awareness/INDEX.md](docs/skilled-awareness/INDEX.md)
- Registry of all 14 capabilities
- Current coverage: 2/14 (14%)
- Target coverage: 100% by Phase 4 (2026-05)
- Dependencies, priorities, effort estimates

**Framework SAP**: [docs/skilled-awareness/sap-framework/](docs/skilled-awareness/sap-framework/)
- Meta-SAP defining the SAP pattern itself
- Complete reference implementation
- Templates and guidelines

**Reference Implementation**: [docs/skilled-awareness/inbox/](docs/skilled-awareness/inbox/)
- Pilot SAP for cross-repo coordination
- Complete example with infrastructure and testing

### Creating SAPs

**When to Create SAP**:
- New major capability (e.g., testing-framework, docker-operations)
- Capability needs structured governance
- Multiple adopters will use capability
- Clear upgrade path needed

**Process**:
1. Read: [docs/skilled-awareness/document-templates.md](docs/skilled-awareness/document-templates.md)
2. Create directory: `docs/skilled-awareness/<capability-name>/`
3. Create 5 artifacts using templates
4. Add infrastructure (schemas, templates, etc.)
5. Update SAP Index: [docs/skilled-awareness/INDEX.md](docs/skilled-awareness/INDEX.md)
6. Follow DDD â†’ BDD â†’ TDD:
   - DDD: Create Charter + Protocol
   - BDD: Define acceptance criteria
   - TDD: Implement infrastructure + Awareness + Blueprint

**Time Estimate**: 8-20 hours per SAP (varies by complexity)

### Installing SAPs

**Process**:
1. Find SAP in INDEX.md
2. Navigate to SAP directory (e.g., `docs/skilled-awareness/inbox/`)
3. Read `adoption-blueprint.md`
4. Execute installation steps sequentially
5. Run validation commands
6. Update `ledger.md` (add adopter record)

**Example**:
```bash
# Find SAP
cat docs/skilled-awareness/INDEX.md

# Read blueprint
cat docs/skilled-awareness/inbox/adoption-blueprint.md

# Execute steps (agent-executable markdown instructions)
# ... follow blueprint step-by-step ...

# Validate
ls inbox/coordination/CAPABILITIES && echo "âœ… Installed"
```

### Validating SAPs (SAP-008 L3)

**Automated SAP structure validation** using [scripts/sap-validate.py](scripts/sap-validate.py):

**Quick Commands**:
```bash
# Validate single SAP
just validate-sap-structure docs/skilled-awareness/testing-framework

# Validate all SAPs
just validate-all-saps

# Or call directly
python scripts/sap-validate.py docs/skilled-awareness/testing-framework
python scripts/sap-validate.py --all
```

**What It Checks**:
- âœ… 5 required artifacts present (charter, protocol, awareness, blueprint, ledger)
- âœ… Valid frontmatter in each artifact (---...---)
- âœ… SAP ID format (SAP-###)
- âœ… Version follows semver (X.Y.Z)
- âœ… Required frontmatter fields (sap_id, version, status)

**Output Example**:
```
[OK] docker-operations
[FAIL] testing-framework
  - capability-charter.md: Missing frontmatter (---...---)
  - protocol-spec.md: Missing frontmatter (---...---)

Summary: 1/2 SAPs passed
```

**Baseline Status** (2025-11-04):
- 2/28 SAPs have proper frontmatter (docker-operations, metrics-tracking)
- 26/28 SAPs need frontmatter added to artifacts
- Target: 100% SAP compliance by end of Phase 3

### SAP Roadmap

**Phase 1** (2025-10 â†’ 2025-11): Framework Hardening
- âœ… SAP-000 (sap-framework)
- âœ… SAP-001 (inbox-coordination, pilot)
- ğŸ”„ SAP-002 (chora-base-meta)

**Phase 2** (2025-11 â†’ 2026-01): Core Capabilities
- SAP-003 (project-bootstrap)
- SAP-004 (testing-framework)
- SAP-005 (ci-cd-workflows)
- SAP-006 (quality-gates)

**Phase 3** (2026-01 â†’ 2026-03): Extended Capabilities
- SAP-007 (documentation-framework)
- SAP-008 (automation-scripts)
- SAP-009 (agent-awareness)
- SAP-010 (memory-system / A-MEM)
- SAP-011 (docker-operations)
- SAP-012 (development-lifecycle)

**Phase 4** (2026-03 â†’ 2026-05): Optimization
- SAP-013 (metrics-tracking)

**See**: [docs/skilled-awareness/chora-base-sap-roadmap.md](docs/skilled-awareness/chora-base-sap-roadmap.md)

---

## Repository Structure

```
chora-base/
â”œâ”€â”€ SKILLED_AWARENESS_PACKAGE_PROTOCOL.md  # Root SAP protocol
â”œâ”€â”€ static-template/             # Static scaffolding copied into generated projects
â”‚   â”œâ”€â”€ src/                     # Python source baseline
â”‚   â”œâ”€â”€ tests/                   # Test baseline
â”‚   â”œâ”€â”€ scripts/                 # Automation scripts
â”‚   â”œâ”€â”€ .github/workflows/       # CI/CD workflows
â”‚   â””â”€â”€ ...                      # Other project files

â”‚   â”œâ”€â”€ README.md.blueprint
â”‚   â”œâ”€â”€ CLAUDE.md.blueprint
â”‚   â”œâ”€â”€ AGENTS.md.blueprint
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/                        # Template documentation
â”‚   â”œâ”€â”€ reference/
â”‚   â”‚   â”œâ”€â”€ skilled-awareness/   # SAP Framework & all SAPs
â”‚   â”‚   â”‚   â”œâ”€â”€ INDEX.md         # SAP registry (all 14 capabilities)
â”‚   â”‚   â”‚   â”œâ”€â”€ document-templates.md  # Templates for creating SAPs
â”‚   â”‚   â”‚   â”œâ”€â”€ chora-base-sap-roadmap.md  # Phased adoption plan
â”‚   â”‚   â”‚   â”œâ”€â”€ sap-framework/   # SAP-000: Framework SAP (meta-capability)
â”‚   â”‚   â”‚   â”œâ”€â”€ inbox/           # SAP-001: Inbox coordination (pilot)
â”‚   â”‚   â”‚   â”œâ”€â”€ chora-base/      # SAP-002: chora-base meta-SAP (planned)
â”‚   â”‚   â”‚   â””â”€â”€ ...              # Future SAPs (project-bootstrap, testing, etc.)
â”‚   â”‚   â”œâ”€â”€ template-configuration.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ upgrades/                # Version upgrade guides
â”‚   â”‚   â”œâ”€â”€ README.md            # Upgrade guide index
â”‚   â”‚   â”œâ”€â”€ PHILOSOPHY.md        # Upgrade philosophy
â”‚   â”‚   â”œâ”€â”€ UPGRADE_GUIDE_TEMPLATE.md  # Template for writing upgrade guides
â”‚   â”‚   â”œâ”€â”€ v1.9.2-to-v1.9.3.md  # Version-specific guides (naming: vX.Y-to-vX.Z.md)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ how-to/                  # Task-oriented guides
â”‚   â”‚   â”œâ”€â”€ 01-setup-new-project.md
â”‚   â”‚   â”œâ”€â”€ 02-configure-testing.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ explanation/             # Conceptual explanations
â”‚   â”œâ”€â”€ research/                # Research documents
â”‚   â”‚   â””â”€â”€ Agentic Coding Best Practices Research.pdf
â”‚   â”œâ”€â”€ BENEFITS.md              # ROI analysis
â”‚   â””â”€â”€ DOCUMENTATION_PLAN.md    # Documentation strategy
â”œâ”€â”€ inbox/                       # Inbox coordination capability (SAP-001)
â”‚   â”œâ”€â”€ INBOX_PROTOCOL.md        # Protocol
â”‚   â”œâ”€â”€ CLAUDE.md                # Awareness
â”‚   â”œâ”€â”€ schemas/                 # JSON schemas
â”‚   â”œâ”€â”€ coordination/            # Coordination directory
â”‚   â””â”€â”€ examples/                # Complete examples (e.g., health-monitoring-w3)
â”œâ”€â”€ examples/                    # Example generated projects
â”‚   â”œâ”€â”€ full-featured-with-vision/
â”‚   â”œâ”€â”€ full-featured-with-docs/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ CHANGELOG.md                 # Version history
â”œâ”€â”€ README.md                    # Project overview
â””â”€â”€ AGENTS.md                    # This file

**IMPORTANT**: This is the FIRST TIME chora-base has its own AGENTS.md. Previously, lack of guidance caused agents to misplace files (e.g., upgrade docs in repo root instead of docs/upgrades/).
```

---

## File Organization Conventions

### Upgrade Documentation

**Location**: `docs/upgrades/`
**Naming Pattern**: `vX.Y-to-vX.Z.md` (e.g., `v1.9.2-to-v1.9.3.md`)
**Not**: UPPERCASE naming, repo root placement

**Why This Matters**: Previous upgrade docs (v1.9.0-to-v1.9.1, v1.9.1-to-v1.9.2) were misplaced in repo root because chora-base lacked this AGENTS.md to guide agents.

**Template**: Use `docs/upgrades/UPGRADE_GUIDE_TEMPLATE.md` for structure

### Research Documents

**Location**: `docs/research/`
**Format**: PDF, markdown, or other documentation formats
**Purpose**: Background research informing template features

### How-To Guides

**Location**: `docs/how-to/`
**Naming Pattern**: `NN-kebab-case-title.md` (e.g., `01-setup-new-project.md`)
**Audience**: Humans + AI agents (task-oriented)

### Reference Documentation

**Location**: `docs/reference/`
**Naming Pattern**: `kebab-case-title.md` (e.g., `template-configuration.md`)
**Audience**: Humans + AI agents (information-oriented)

### Example Projects

**Location**: `examples/`
**Structure**: Full generated project directories
**Purpose**: Test template features, demonstrate usage patterns
**Status**: Examples may lag behind template (document drift in commit messages)

---

## Development Workflows

### Adding New Features to Template

1. **Research Phase**
   - Document research findings in `docs/research/`
   - Identify industry best practices
   - Review adopter feedback from real projects (mcp-n8n, chora-compose, and others across MCP, REST, CLI, library domains)

2. **Design Phase**
   - Update `CHANGELOG.md` under `## [Unreleased]`
   - Consider opt-in vs opt-out (new features usually opt-in)
   - Determine whether capability needs its own Skilled Awareness Package (SAP)
   - Update relevant blueprints and static assets to include the feature

3. **Implementation Phase**
   - Update `static-template/` and `blueprints/` assets
   - Add automation or install scripts if feature is optional
   - Run blueprint generation scripts (or agent-assisted flow) to validate outputs
   - Generate/update examples in `examples/` if demonstrating complex feature

4. **Documentation Phase**
   - Update `README.md` if user-facing feature
   - Add to `template/AGENTS.md.jinja` if relevant to AI agents
   - Document in appropriate `docs/` subdirectory
   - Update `docs/BENEFITS.md` if adding measurable value
   - Create/refresh SAP documents (charter, protocol, awareness guide, adoption blueprint, ledger entry)

5. **Validation Phase**
   - Generate fresh project using blueprint workflow (see setup guides)
   - Run generated project's tests: `cd /tmp/test-validation && ./scripts/setup.sh && pytest`
   - Test upgrade path: follow SAP adoption blueprint scenarios (e.g., apply updates to example repos)
   - Verify conditional combinations (Docker on/off, memory on/off, etc.)

### Releasing New Version

**Version Numbering**: Semantic versioning (MAJOR.MINOR.PATCH)
- **MAJOR** (X.0.0): Breaking changes requiring adopter action
- **MINOR** (1.X.0): New features, additive changes (current: v1.9.3)
- **PATCH** (1.1.X): Bug fixes only

**Release Process**:

1. **Pre-Release Validation**
   ```bash
   # Generate project from blueprints (interactive; answer prompts as needed)
   # Use AI agent to generate projects from static-template/ and SAP templates
   cd /tmp/test-release && ./scripts/setup.sh && pytest

   # Test update path (use example project + SAP adoption blueprint)
   cd examples/full-featured-with-vision
   # Follow relevant SAP adoption blueprint to apply latest changes
   ./scripts/setup.sh && pytest
   ```

2. **Create Upgrade Guide**
   - Location: `docs/upgrades/vX.Y-to-vX.Z.md`
   - Use template: `docs/upgrades/UPGRADE_GUIDE_TEMPLATE.md`
   - Include decision tree for AI agents
   - Document displacement risk (ZERO/LOW/MEDIUM/HIGH)
   - Add migration steps, validation checklist

3. **Update Core Files**
   ```bash
   # Update CHANGELOG.md
   # Change: ## [Unreleased]
   # To:     ## [X.Y.Z] - 2025-MM-DD

   # Update README.md Recent Updates section
   # Add new version entry with key features

   # Update docs/upgrades/README.md
   # Add version to version-specific guides table
   ```

4. **Commit and Tag**
   ```bash
   git add -A
   git commit -m "feat(scope): Brief description (vX.Y.Z)

   Detailed changes:
   - Feature 1
   - Feature 2

   ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

   Co-Authored-By: Claude <noreply@anthropic.com>"

   git tag vX.Y.Z
   git push origin main --tags
   ```

5. **Create GitHub Release**
   ```bash
   gh release create vX.Y.Z \
     --title "vX.Y.Z - Brief Title" \
     --notes "Release notes here (benefits, changes, upgrade guide link)"
   ```

6. **Update docs/upgrades/README.md**
   - Update version table with new entry
   - Update "Status" line if completing a phase

### Creating Upgrade Documentation

**Template**: `docs/upgrades/UPGRADE_GUIDE_TEMPLATE.md`

**Required Sections**:
1. Quick Assessment (TL;DR, effort estimate)
2. Decision Tree for AI Agents (structured IF/THEN)
3. What Changed (file-by-file with line counts)
4. Migration Steps (bash commands)
5. Testing After Upgrade (validation checklist)
6. Rollback Instructions
7. FAQ

**AI-Optimized Format**:
- Machine-parseable decision criteria
- Clear displacement risk assessment (ZERO/LOW/MEDIUM/HIGH)
- Structured upgrade effort estimate (<10min, 10-20min, 1-2hrs, etc.)
- Benefits vs costs analysis for optional changes

---

## Testing Strategy

### Template Generation Tests

```bash
# Minimal configuration (accept defaults)
# Use AI agent to generate projects from static-template/ and SAP templates

# Full-featured configuration
# Use AI agent to generate projects from static-template/ and SAP templates
# Enable all optional features when prompted

# Specific project types (run setup script and choose appropriate options)
# Use AI agent to generate projects from static-template/ and SAP templates
# Use AI agent to generate projects from static-template/ and SAP templates
# Use AI agent to generate projects from static-template/ and SAP templates
```

### Generated Project Validation

```bash
# Validate generated project can setup and test
cd /tmp/test-project
./scripts/setup.sh
pytest
pre-commit run --all-files
just test  # if justfile included
```

### Update Path Testing

```bash
# Test template updates merge correctly (use SAP adoption blueprint)
cd examples/full-featured-with-vision
git checkout -b test-update
# Apply latest blueprint changes manually or via automation scripts
# Resolve any conflicts and verify diff is reasonable
```

### Conditional Feature Testing

Test feature combinations that users commonly choose:

1. **Minimal Python Project**: Basic structure, no optional features
2. **Full-Featured Python Project**: All optional features enabled
3. **Library Project**: No CLI, yes docs, yes tests
4. **CLI Tool Project**: Yes CLI, yes tests
5. **MCP Server** (with SAP-014): MCP-specific features, optional memory/Docker

### Quality Gates (Pre-commit Hooks) - SAP-006 L3

**Purpose**: Automated code quality enforcement via pre-commit hooks (ruff, mypy, black), catching 95%+ preventable issues locally before CI.

**Adoption Level**: L3 (Fully automated, production-ready)

**Core Quality Gates**:
- **ruff**: Fast Python linting (10-100x faster than flake8)
- **mypy**: Static type checking (catch type errors pre-runtime)
- **black**: Automated code formatting (consistent style)
- **trailing-whitespace**: Remove trailing whitespace
- **end-of-file-fixer**: Ensure files end with newline

**Hook Execution Flow**:
```bash
# 1. Stage files
git add file.py

# 2. Attempt commit
git commit -m "Add feature"

# 3. Pre-commit hooks run automatically (in order):
#    a. ruff check (linting)
#    b. mypy (type checking)
#    c. black (formatting)
#    d. trailing-whitespace
#    e. end-of-file-fixer

# 4a. If hooks pass â†’ Commit succeeds
# 4b. If hooks fail â†’ Commit blocked, files modified, re-stage and retry

# 5. Re-stage modified files (if hooks auto-fixed)
git add file.py
git commit -m "Add feature"  # Try again
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Verify pre-commit is installed
pre-commit --version || pip install pre-commit

# 2. Install hooks (if not already installed)
pre-commit install

# 3. Run hooks on all files to check status
pre-commit run --all-files
just pre-commit-all

# 4. Fix any issues reported
ruff check --fix src/ tests/ scripts/
just lint-fix

# 5. Verify type checking passes
mypy src/ tests/ scripts/
just typecheck
```

**Common Workflows**:

**Workflow 1: Normal commit (hooks pass)**:
```bash
# Make changes
vim src/module.py

# Stage and commit
git add src/module.py
git commit -m "Add feature"  # Hooks run, pass, commit succeeds
```

**Workflow 2: Hooks fail (linting errors)**:
```bash
# Make changes
vim src/module.py

# Stage and commit
git add src/module.py
git commit -m "Add feature"  # Hooks fail with linting errors

# Fix linting automatically
ruff check --fix src/module.py
just lint-fix

# Re-stage and commit
git add src/module.py
git commit -m "Add feature"  # Hooks pass, commit succeeds
```

**Workflow 3: Hooks fail (type errors)**:
```bash
# Make changes
vim src/module.py

# Stage and commit
git add src/module.py
git commit -m "Add feature"  # Hooks fail with type errors

# Fix type errors manually
vim src/module.py  # Add type annotations

# Re-stage and commit
git add src/module.py
git commit -m "Add feature"  # Hooks pass, commit succeeds
```

**Workflow 4: Emergency bypass (NOT recommended)**:
```bash
# Only use in emergency (hotfix, urgent production issue)
git commit -m "Hotfix" --no-verify  # Skip hooks

# Create follow-up task to fix quality issues
bd create "Fix quality issues from emergency commit" --priority high
```

**Hook Configuration** (.pre-commit-config.yaml):
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: https://github.com/psf/black
    rev: 23.11.0
    hooks:
      - id: black

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
```

**Integration with Other SAPs**:
- **SAP-005 (CI/CD)**: Dual validation (pre-commit local + CI remote)
  - Pre-commit: Fast (<5s), runs on every commit
  - CI: Comprehensive (matrix testing), runs on push/PR
  - Pattern: Pre-commit catches 95%+ issues, CI catches edge cases

- **SAP-004 (Testing)**: Optional pytest hook
  ```yaml
  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
  ```

- **SAP-031 (Enforcement)**: Pre-commit as Layer 2 enforcement
  - Layer 1: Discoverability (70% prevention via patterns in AGENTS.md)
  - Layer 2: Pre-commit hooks (20% prevention via automated checks)
  - Layer 3: CI/CD (9% prevention via comprehensive testing)

**Troubleshooting**:

1. **Hooks not running**:
   ```bash
   # Re-install hooks
   pre-commit uninstall
   pre-commit install
   ```

2. **Hooks failing after update**:
   ```bash
   # Update hook versions
   pre-commit autoupdate
   just pre-commit-update
   ```

3. **Ruff linting errors**:
   ```bash
   # Auto-fix fixable issues
   ruff check --fix src/ tests/ scripts/
   just lint-fix

   # Manual fix required for complex issues
   ruff check src/ tests/ scripts/  # See error details
   ```

4. **Mypy type errors**:
   ```bash
   # Run mypy to see errors
   mypy src/ tests/ scripts/
   just typecheck

   # Add type annotations or suppressions
   # Example: # type: ignore[arg-type]
   ```

**L3 Achievement Evidence** (2025-11-09):
- âœ… Pre-commit hooks installed (.pre-commit-config.yaml)
- âœ… 5 hooks configured (ruff, mypy, black, trailing-whitespace, end-of-file-fixer)
- âœ… Fast execution (<5s for typical commits)
- âœ… CI integration (lint.yml workflow validates same checks)
- âœ… Justfile recipes (6 quality commands)
- âœ… Dual validation (local + CI) prevents 95%+ preventable issues

**ROI Metrics**:
- **Issue prevention**: 95%+ preventable issues caught locally
- **Time savings**: <5s local checks vs 5-10 min CI failure cycle
- **CI cost reduction**: 90% fewer CI failures (avoid wasted CI minutes)
- **Code quality**: Consistent style, type safety, reduced bugs

**Documentation**:
- Protocol specification: [docs/skilled-awareness/quality-gates/protocol-spec.md](docs/skilled-awareness/quality-gates/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/quality-gates/adoption-blueprint.md](docs/skilled-awareness/quality-gates/adoption-blueprint.md)
- Hook configuration: [.pre-commit-config.yaml](.pre-commit-config.yaml)

---

### Project Bootstrap (Fast Setup Script) - SAP-003 L3

**Purpose**: Provide 1-2 minute automated project generation using fast-setup script, creating fully-configured MCP servers with all chora-base infrastructure.

**Adoption Level**: L3 (Production-ready template, battle-tested)

**Core Fast-Setup Workflow**:
```bash
# 1. Execute fast-setup script
python scripts/create-model-mcp-server.py \
    --name "Your Project Name" \
    --namespace yournamespace \
    --output ~/projects/your-project

# 2. Fast-setup automatically:
# - Copies chora-base template to output directory
# - Substitutes variables (name, namespace, Python package name)
# - Generates directory structure (src/, tests/, docs/, .chora/, .beads/)
# - Initializes git repository with provenance commit
# - Installs dependencies (pip install -e .)
# - Installs pre-commit hooks (quality gates)
# - Runs initial tests (pytest verification)
# - Generates project-specific README.md

# 3. Result: Fully-functional MCP server (1-2 min)
cd ~/projects/your-project
pytest                              # All tests pass (100%)
just test                           # pytest with 85%+ coverage
git log                             # Fast-setup provenance in commit
```

**Session Startup Routine** (agents should execute):
```bash
# 1. If user wants to create new project â†’ Use fast-setup
python scripts/create-model-mcp-server.py --help

# 2. Verify template integrity before fast-setup
just verify-template

# 3. After fast-setup â†’ Validate generated project
just validate-project ~/projects/your-project

# 4. Optional: List available templates
just list-templates
```

**Common Workflows**:

1. **Create new MCP server project**:
```bash
# Use fast-setup script with required parameters
python scripts/create-model-mcp-server.py \
    --name "Weather Data MCP" \
    --namespace weatherdata \
    --output ~/projects/weather-mcp

# Verify project generated successfully
cd ~/projects/weather-mcp
pytest --cov=src --cov-fail-under=85
just test
```

2. **Verify template integrity** (before fast-setup):
```bash
# Check all required template files exist
just verify-template

# Expected output:
# âœ… Template integrity verified
```

3. **Validate generated project** (after fast-setup):
```bash
# Run full validation suite on generated project
just validate-project ~/projects/weather-mcp

# Validation includes:
# - pytest with 85%+ coverage gate
# - ruff linting checks
# - mypy type checking
```

4. **Test fast-setup script** (development/debugging):
```bash
# Dry-run mode: Show help without creating project
just test-fast-setup

# Output: Fast-setup script help and usage
```

5. **Customize generated project** (post-fast-setup):
```bash
cd ~/projects/weather-mcp

# 1. Update README.md with project-specific details
# 2. Add MCP tools/resources in src/weatherdata/server.py
# 3. Write tests in tests/
# 4. Configure CI/CD in .github/workflows/
# 5. Update dependencies in pyproject.toml
# 6. Commit changes
git add .
git commit -m "feat: Add weather data MCP tools"
```

**Integration with Other SAPs**:
- **SAP-000 (Framework)**: Fast-setup adopts all core SAPs automatically (6 SAPs)
- **SAP-004 (Testing)**: pytest framework pre-configured with 85%+ coverage gate
- **SAP-005 (CI/CD)**: GitHub Actions workflows pre-installed (.github/workflows/)
- **SAP-006 (Quality Gates)**: Pre-commit hooks pre-installed and enabled
- **SAP-001 (Inbox)**: Coordination protocol files generated (inbox/coordination/)
- **SAP-010 (Memory)**: A-MEM directory structure created (.chora/memory/)
- **SAP-015 (Beads)**: Task tracking initialized (.beads/)
- **SAP-014 (MCP Server)**: FastMCP framework integrated with server entry point

**Troubleshooting**:

| Issue | Symptom | Fix |
|-------|---------|-----|
| Template file missing | `FileNotFoundError` during fast-setup | Run `just verify-template` to check integrity |
| Fast-setup script not found | `python scripts/create-model-mcp-server.py` fails | Verify you're in chora-base root directory |
| Generated project tests fail | `pytest` fails in generated project | Run `just validate-project <path>` to diagnose |
| Import errors after fast-setup | `ModuleNotFoundError` in generated project | Run `pip install -e .` in generated project |
| Pre-commit hooks not installed | Commits succeed without quality checks | Run `pre-commit install` in generated project |

**L3 Achievement Evidence**:
- âœ… Fast-setup script tested across 5+ generated projects (100% success rate)
- âœ… Template integrity validation automated (just verify-template)
- âœ… Generated projects pass all quality gates out-of-the-box
- âœ… Execution time: 1-2 minutes (95% faster than manual setup)
- âœ… Model citizen pattern: Zero-config production readiness

**ROI Metrics**:
- **Time savings**: 95% reduction (30-60 min manual â†’ 1-2 min fast-setup)
- **Zero-config deployment**: Generated projects pass all quality gates immediately
- **SAP adoption**: 6+ SAPs adopted automatically (vs manual adoption: 2-3 hours)
- **Production readiness**: 100% of generated projects are model citizens

**Documentation**:
- Quickstart guide: [docs/user-docs/quickstart-mcp-server.md](docs/user-docs/quickstart-mcp-server.md)
- Protocol specification: [docs/skilled-awareness/project-bootstrap/protocol-spec.md](docs/skilled-awareness/project-bootstrap/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/project-bootstrap/adoption-blueprint.md](docs/skilled-awareness/project-bootstrap/adoption-blueprint.md)
- Fast-setup script: [scripts/create-model-mcp-server.py](scripts/create-model-mcp-server.py)

---

### Chora-Base Meta Package (Documentation Framework) - SAP-002 L3

**Purpose**: Meta-capability that describes chora-base itself using the SAP framework, demonstrating self-documenting architecture through dogfooding.

**Adoption Level**: L3 (Self-describing, production-ready documentation framework)

**Core Documentation Framework**:
```bash
# Chora-base uses 4-domain documentation structure:
docs/
â”œâ”€â”€ user-docs/           # Getting started, tutorials, how-to guides
â”œâ”€â”€ dev-docs/            # Architecture, contributing, development
â”œâ”€â”€ project-docs/        # Plans, decisions, retrospectives
â””â”€â”€ skilled-awareness/   # SAP capabilities (30+ modular packages)

# Root-level awareness files:
AGENTS.md                # Generic agent patterns
CLAUDE.md                # Claude-specific navigation
sap-catalog.json         # Machine-readable SAP catalog
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Understand what chora-base is
cat README.md                         # Overview, features, quick start

# 2. Explore SAP catalog (30+ capabilities)
just list-saps                        # List all available SAPs
cat sap-catalog.json                  # Machine-readable catalog
cat docs/skilled-awareness/INDEX.md   # Human-readable index

# 3. Check chora-base structure
just verify-structure                 # Validate integrity
just chora-info                       # Show meta information

# 4. Navigate to relevant domain
ls docs/user-docs/                    # User guides, tutorials
ls docs/dev-docs/                     # Development setup
ls docs/project-docs/                 # Plans, decisions
ls docs/skilled-awareness/            # SAP capabilities
```

**Common Workflows**:

1. **Understand chora-base architecture**:
```bash
# Read root awareness files
cat AGENTS.md                         # Agent patterns
cat CLAUDE.md                         # Claude navigation

# Explore documentation domains
cat docs/user-docs/AGENTS.md          # User documentation patterns
cat docs/dev-docs/AGENTS.md           # Developer patterns
cat docs/project-docs/AGENTS.md       # Project management patterns
cat docs/skilled-awareness/AGENTS.md  # SAP patterns
```

2. **Browse SAP catalog**:
```bash
# Machine-readable catalog
just list-saps                        # List all SAPs with status
cat sap-catalog.json | jq '.saps[] | {id, name, status, version}'

# Human-readable index
cat docs/skilled-awareness/INDEX.md   # Categorized SAP list
```

3. **Explore 4-domain documentation**:
```bash
# User documentation (getting started, tutorials)
just explore-docs                     # Show documentation structure
ls docs/user-docs/                    # List user guides
cat docs/user-docs/quickstart-mcp-server.md

# Developer documentation (architecture, contributing)
ls docs/dev-docs/                     # List developer docs
cat docs/dev-docs/ARCHITECTURE.md

# Project documentation (plans, decisions, retrospectives)
ls docs/project-docs/                 # List project docs
ls docs/project-docs/plans/           # Development plans

# SAP documentation (30+ modular capabilities)
ls docs/skilled-awareness/            # List all SAPs
cat docs/skilled-awareness/INDEX.md   # SAP catalog
```

4. **Verify chora-base structure**:
```bash
# Automated integrity check
just verify-structure                 # Validate required dirs/files

# Manual verification
ls -la                                # Root files (AGENTS.md, CLAUDE.md, etc.)
ls docs/                              # 4 documentation domains
ls src/                               # Source code
ls tests/                             # Test suite
ls scripts/                           # Automation scripts
```

5. **Learn SAP framework by example** (meta-pattern):
```bash
# Chora-base IS a SAP (SAP-002), demonstrating SAP framework
ls docs/skilled-awareness/chora-base/
# - capability-charter.md (problem statement, solution design)
# - protocol-spec.md (complete technical specification)
# - awareness-guide.md (operating patterns)
# - adoption-blueprint.md (installation guide)
# - ledger.md (metrics, feedback, version history)

# Compare with other SAPs to see consistent pattern
ls docs/skilled-awareness/testing-framework/
ls docs/skilled-awareness/ci-cd-workflows/
ls docs/skilled-awareness/quality-gates/
```

**Integration with Other SAPs**:
- **SAP-000 (Framework)**: chora-base implements all SAP framework requirements (5 artifacts)
- **SAP-009 (Awareness)**: chora-base uses nested AGENTS.md/CLAUDE.md hierarchy extensively
- **SAP-003 (Bootstrap)**: Fast-setup script generates projects from chora-base template
- **SAP-027 (Dogfooding)**: chora-base validates SAP patterns through self-application
- **ALL SAPs**: chora-base provides the foundation that all other SAPs build upon

**Troubleshooting**:

| Issue | Symptom | Fix |
|-------|---------|-----|
| Missing documentation files | `FileNotFoundError` when reading docs | Run `just verify-structure` to check integrity |
| SAP catalog not found | Cannot list SAPs | Verify `sap-catalog.json` exists in root directory |
| Documentation domain confusion | Unclear which docs/ subdirectory to use | Read [CLAUDE.md](CLAUDE.md) navigation tree (lines 174-365) |
| Can't find specific SAP | Looking for SAP-XXX documentation | Use `just list-saps` or read [docs/skilled-awareness/INDEX.md](docs/skilled-awareness/INDEX.md) |
| Outdated documentation | Documentation doesn't match current version | Check git log and version in README.md |

**L3 Achievement Evidence**:
- âœ… 4-domain documentation structure implemented and documented
- âœ… 30+ SAPs packaged using consistent SAP framework pattern
- âœ… Self-describing architecture (chora-base AS a SAP demonstrates SAP framework)
- âœ… Progressive context loading via nested AGENTS.md/CLAUDE.md hierarchy
- âœ… Machine-readable catalog (sap-catalog.json) for programmatic access

**ROI Metrics**:
- **Documentation clarity**: 4 domains with clear separation of concerns
- **SAP reusability**: 30+ modular capabilities for adoption
- **Self-documenting**: Zero documentation debt (chora-base documents itself)
- **Agent discoverability**: Progressive context loading saves 60-70% tokens
- **Time savings**: 52+ hours per project via pre-configured infrastructure

**Documentation**:
- Benefits guide: [docs/user-docs/explanation/benefits-of-chora-base.md](docs/user-docs/explanation/benefits-of-chora-base.md)
- Architecture overview: [docs/dev-docs/AGENTS.md](docs/dev-docs/AGENTS.md)
- Protocol specification: [docs/skilled-awareness/chora-base/protocol-spec.md](docs/skilled-awareness/chora-base/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/chora-base/adoption-blueprint.md](docs/skilled-awareness/chora-base/adoption-blueprint.md)
- SAP catalog: [sap-catalog.json](sap-catalog.json), [docs/skilled-awareness/INDEX.md](docs/skilled-awareness/INDEX.md)

---

### Documentation Framework (DiÃ¡taxis 4-Domain) - SAP-007 L3

**Purpose**: Provide DiÃ¡taxis-based documentation architecture with frontmatter validation, executable how-to guides, and Level 3 enforcement layer.

**Adoption Level**: L3 (Fully enforced via pre-commit hooks and CI, SAP-031 integration)

**Core Documentation Framework**:
```bash
# DiÃ¡taxis 4-domain structure:
docs/
â”œâ”€â”€ user-docs/
â”‚   â”œâ”€â”€ tutorials/        # Learning-oriented (step-by-step lessons)
â”‚   â”œâ”€â”€ how-to/           # Task-oriented (practical guides)
â”‚   â”œâ”€â”€ explanation/      # Understanding-oriented (concepts, design rationale)
â”‚   â””â”€â”€ reference/        # Information-oriented (API docs, technical specs)
â”œâ”€â”€ dev-docs/             # Developer documentation (architecture, contributing)
â”œâ”€â”€ project-docs/         # Project management (plans, decisions, retrospectives)
â””â”€â”€ skilled-awareness/    # SAP capabilities (30+ modular packages)
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Check documentation structure
just doc-structure                # Show 4-domain hierarchy

# 2. Validate documentation standards
just validate-docs                # Run DOCUMENTATION_STANDARD.md checks
just validate-frontmatter         # Check YAML frontmatter schema

# 3. Check completeness
just doc-completeness             # Verify all 4 domains exist

# 4. Extract tests from docs (if writing how-to guides)
just extract-doc-tests            # Generate tests from code blocks
```

**Common Workflows**:

1. **Create new how-to guide**:
```bash
# Use frontmatter schema (required fields)
cat > docs/user-docs/how-to/new-guide.md <<'EOF'
---
audience: [developers, agents]
time: 10 minutes
prerequisites: [Python 3.11+, git]
difficulty: intermediate
related: [other-guide.md]
---

# How to [Task Name]

**Goal**: [Clear objective in 1 sentence]

## Quick Start

\```bash
# Step 1: [Description]
command-1

# Step 2: [Description]
command-2
\```

## Detailed Steps

### Step 1: [Name]
[Explanation]

### Step 2: [Name]
[Explanation]

## Troubleshooting

**Problem**: [Common issue]
**Solution**: [Fix]
EOF

# Validate frontmatter
just validate-frontmatter

# Extract tests
just extract-doc-tests
```

2. **Validate documentation**:
```bash
# Run all validation checks
just validate-docs                # DOCUMENTATION_STANDARD.md compliance
just validate-frontmatter         # YAML schema validation

# Pre-commit hooks (L3 enforcement)
pre-commit run --all-files        # Includes doc validation
```

3. **Organize documentation using DiÃ¡taxis**:
```bash
# Decision tree: Where does this doc belong?
# 1. Is it learning-oriented? â†’ tutorials/
# 2. Is it task-oriented? â†’ how-to/
# 3. Is it understanding-oriented? â†’ explanation/
# 4. Is it information-oriented? â†’ reference/

# Example decisions:
# "Getting Started with chora-base" â†’ tutorials/ (learning)
# "How to Add a New SAP" â†’ how-to/ (task)
# "Why DiÃ¡taxis?" â†’ explanation/ (understanding)
# "SAP Framework API" â†’ reference/ (information)

# Show current structure
just doc-structure
```

4. **Extract tests from how-to guides**:
```bash
# Extract code blocks from how-to guides as tests
just extract-doc-tests

# Generated tests location
ls tests/extracted/               # Test files from docs

# Run extracted tests
pytest tests/extracted/           # Verify docs are executable
```

5. **Check documentation completeness**:
```bash
# Verify all 4 DiÃ¡taxis domains exist
just doc-completeness

# Expected output:
# âœ… Tutorials
# âœ… How-To Guides
# âœ… Explanations
# âœ… References
```

**Integration with Other SAPs**:
- **SAP-031 (Enforcement)**: Documentation validation as Layer 3 enforcement (5-10% prevention)
- **SAP-006 (Quality Gates)**: Pre-commit hooks validate doc structure and frontmatter
- **SAP-004 (Testing)**: Extracted doc tests run in pytest suite (docs as living tests)
- **SAP-002 (Chora-Base Meta)**: 4-domain structure demonstrated in chora-base itself
- **SAP-000 (Framework)**: Every SAP follows DiÃ¡taxis pattern in its 5 artifacts

**Troubleshooting**:

| Issue | Symptom | Fix |
|-------|---------|-----|
| Frontmatter validation fails | YAML syntax error in how-to guide | Check required fields: audience, time, prerequisites, difficulty |
| Test extraction fails | Code blocks not generating tests | Ensure bash/python code blocks in how-to guides have valid syntax |
| Doc validation fails | DOCUMENTATION_STANDARD.md violations | Read [DOCUMENTATION_STANDARD.md](DOCUMENTATION_STANDARD.md) for rules |
| Missing domain | doc-completeness shows âŒ | Create missing directory (e.g., `mkdir -p docs/user-docs/tutorials`) |
| Pre-commit hook fails | Doc validation in pre-commit | Run `just validate-docs` to see specific issues |

**L3 Achievement Evidence**:
- âœ… DiÃ¡taxis 4-domain structure implemented (tutorials, how-to, explanation, reference)
- âœ… Frontmatter schema validated via pre-commit hooks (100% coverage)
- âœ… DOCUMENTATION_STANDARD.md enforced (700 lines, comprehensive guide)
- âœ… Test extraction functional (80%+ syntax accuracy, 60%+ semantic accuracy)
- âœ… Level 3 enforcement via SAP-031 (pre-commit + CI validation)

**ROI Metrics**:
- **Documentation quality**: 40-60% improvement (standardized structure, validated frontmatter)
- **Time savings**: 15-20 min per session (avoid doc inconsistencies, find docs faster)
- **Test coverage**: +5-10% from extracted doc tests (docs as living tests)
- **Discoverability**: 4-domain structure improves navigation (2-3x faster doc discovery)

**Documentation**:
- Standard: [DOCUMENTATION_STANDARD.md](DOCUMENTATION_STANDARD.md) (700 lines, comprehensive guide)
- Protocol specification: [docs/skilled-awareness/documentation-framework/protocol-spec.md](docs/skilled-awareness/documentation-framework/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/documentation-framework/adoption-blueprint.md](docs/skilled-awareness/documentation-framework/adoption-blueprint.md)
- DiÃ¡taxis reference: https://diataxis.fr/

---

### Automation Scripts (25 Script Toolkit) - SAP-008 L1

**Purpose**: Provide 25 automation scripts (shell + Python) organized in 8 categories with justfile unified interface, idempotent operations, and safety contracts.

**Adoption Level**: L1 (Fully operational)

**Core Automation Framework**:
```bash
# 8 script categories:
scripts/
â”œâ”€â”€ Category 1: Setup & Environment
â”‚   â”œâ”€â”€ setup.sh, venv-create.sh, venv-clean.sh, check-env.sh
â”œâ”€â”€ Category 2: Development Workflows
â”‚   â”œâ”€â”€ dev-server.sh, smoke-test.sh, integration-test.sh, diagnose.sh
â”œâ”€â”€ Category 3: Version Management
â”‚   â”œâ”€â”€ bump-version.sh, prepare-release.sh
â”œâ”€â”€ Category 4: Release & Publishing
â”‚   â”œâ”€â”€ build-dist.sh, publish-test.sh, publish-prod.sh, verify-stable.sh
â”œâ”€â”€ Category 5: Safety & Recovery
â”‚   â”œâ”€â”€ rollback-dev.sh, pre-merge.sh
â”œâ”€â”€ Category 6: Documentation
â”‚   â”œâ”€â”€ validate_docs.py, extract_tests.py, docs_metrics.py, generate_docs_map.py, query_docs.py
â”œâ”€â”€ Category 7: MCP & Specialized
â”‚   â”œâ”€â”€ mcp-tool.sh, validate_mcp_names.py
â””â”€â”€ Category 8: Migration & Handoff
    â”œâ”€â”€ migrate_namespace.sh, handoff.sh
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Check available automation commands
just automation-help                # Show all 30+ commands grouped by category
just --list                         # Show all commands

# 2. Validate environment
just check-env                      # Check prerequisites (Python, Git, etc.)
just diagnose                       # Run diagnostics if issues found

# 3. Run quick health check
just smoke                          # Quick smoke tests (~5-10s)
```

**Common Workflows**:

1. **Setup new development environment**:
```bash
# Install project in editable mode
just install                        # pip install -e ".[dev]"

# Install pre-commit hooks
just setup-hooks                    # pre-commit install

# Validate environment
just check-env                      # Check Python, Git, dependencies

# Run smoke tests
just smoke                          # Quick validation (<10s)
```

2. **Development workflow (test â†’ lint â†’ format â†’ type-check)**:
```bash
# Run tests with coverage
just test                           # pytest with coverage report

# Lint code
just lint                           # ruff check (find issues)
just lint-fix                       # ruff check --fix (auto-fix)

# Format code
just format                         # ruff format

# Type checking
just type-check                     # mypy static analysis

# All quality gates before merge
just pre-merge                      # Run all checks (test + lint + format + type-check)
```

3. **Version bump and release**:
```bash
# Bump version (semver)
just bump-patch                     # 1.0.0 â†’ 1.0.1 (bug fixes)
just bump-minor                     # 1.0.0 â†’ 1.1.0 (new features)
just bump-major                     # 1.0.0 â†’ 2.0.0 (breaking changes)

# Build distribution packages
just build                          # Build wheel and sdist

# Publish to test PyPI (validate first)
just publish-test                   # Publish to test.pypi.org

# Publish to production PyPI
just publish-prod                   # Publish to pypi.org
```

4. **Documentation workflows**:
```bash
# Validate documentation standards
just validate-docs                  # Run DOCUMENTATION_STANDARD.md checks

# Validate frontmatter schema
just validate-frontmatter           # Check YAML frontmatter

# Extract tests from how-to guides
just extract-doc-tests              # Generate tests from code blocks

# Show documentation structure
just doc-structure                  # Display DiÃ¡taxis 4-domain hierarchy

# Check completeness
just doc-completeness               # Verify all 4 domains exist
```

5. **Safety and recovery**:
```bash
# Run all pre-merge quality gates
just pre-merge                      # Test + lint + format + type-check

# Generate handoff checklist
just handoff                        # Create handoff report

# Rollback development changes (if needed)
just rollback-dev                   # Restore to clean state
```

6. **Diagnostics and troubleshooting**:
```bash
# Diagnose environment issues
just diagnose                       # Run comprehensive diagnostics

# Check specific prerequisites
just check-env                      # Validate Python, Git, dependencies

# Run smoke tests
just smoke                          # Quick health check

# Run integration tests
just integration                    # Full integration test suite
```

**Integration with Other SAPs**:
- **SAP-012 (Python Patterns)**: Scripts follow Python best practices for shell/Python automation
- **SAP-006 (Quality Gates)**: `just pre-merge` orchestrates all quality gates (test + lint + format + type-check)
- **SAP-005 (CI/CD)**: Scripts power GitHub Actions workflows (test.yml, release.yml, quality-gates.yml)
- **SAP-007 (Documentation)**: Documentation scripts validate DiÃ¡taxis structure and extract tests
- **SAP-014 (MCP Server)**: MCP-specific automation scripts for server development

**Troubleshooting**:

| Issue | Diagnostic | Fix |
|-------|-----------|-----|
| Script not found | `just automation-help` | Check script exists in scripts/ |
| Permission denied | `ls -la scripts/*.sh` | `chmod +x scripts/*.sh` |
| Environment issues | `just diagnose` | Follow diagnostics output |
| Pre-commit fails | `just pre-merge` | Fix issues before commit |
| Test failures | `just test` | Review pytest output |

**L1 Achievement Evidence**:
- âœ… 25 automation scripts operational (shell + Python)
- âœ… justfile with 30+ commands (unified interface)
- âœ… Idempotent operations (scripts can run multiple times safely)
- âœ… Safety contracts (rollback mechanisms for critical operations)
- âœ… 8 script categories (setup, dev, version, release, safety, docs, MCP, migration)

**ROI Metrics**:
- **Time savings**: 30-45 min per day (consistent automation, no manual workflows)
- **Error reduction**: 90%+ reduction in setup errors (idempotent + validation)
- **Consistency**: 100% standardized workflows (all teams use same commands)
- **Discoverability**: `just automation-help` provides instant command reference

---

### Docker Operations (Production Containerization) - SAP-011 L1

**Purpose**: Provide production-ready Docker containerization with multi-stage builds, CI-optimized test containers, docker-compose orchestration, and 40% smaller images.

**Adoption Level**: L1 (Fully operational)

**Core Docker Architecture**:
```bash
# 5 Docker artifacts:
project/
â”œâ”€â”€ Dockerfile                     # Production multi-stage build (150-250MB)
â”œâ”€â”€ Dockerfile.test                # CI test environment (editable install)
â”œâ”€â”€ docker-compose.yml             # Service orchestration
â”œâ”€â”€ .dockerignore                  # Build context optimization (81% reduction)
â””â”€â”€ DOCKER_BEST_PRACTICES.md       # Guidance and troubleshooting

# Multi-stage production build pattern:
# Stage 1: Builder (build wheel)
# Stage 2: Runtime (install wheel, non-root, health check)
# Result: 150-250MB images (vs 500MB+ with editable install)

# CI-optimized test pattern:
# Single-stage: editable install + dev dependencies
# GitHub Actions cache: 6x faster builds (3 min â†’ 30 sec)
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Check if Docker artifacts exist
ls -la Dockerfile Dockerfile.test docker-compose.yml .dockerignore

# 2. Build production image
just docker-build myproject latest

# 3. Build test image (for CI)
just docker-build-test myproject test

# 4. Run tests in container (validate)
just docker-test myproject

# 5. Start services with compose
just docker-up
just docker-health
```

**Common Workflows**:

1. **Build and run production container**:
```bash
# Build production image (multi-stage)
just docker-build myproject latest

# Run production container
docker run -d \
  -p 8000:8000 \
  -v ./configs:/app/configs:ro \
  -v ./logs:/app/logs \
  --name myproject-prod \
  myproject:latest

# Check health
docker inspect myproject-prod | grep -A 10 Health

# View logs
docker logs -f myproject-prod

# Stop and remove
docker stop myproject-prod && docker rm myproject-prod
```

2. **Build and test in CI (GitHub Actions integration)**:
```bash
# Build CI test image (with GitHub Actions cache)
just docker-build-test myproject test

# Run tests in container
just docker-test myproject

# Extract coverage report (if needed)
docker cp $(docker create --name tmp myproject:test):/app/htmlcov ./htmlcov
docker rm tmp
```

3. **Docker Compose orchestration (multi-service)**:
```bash
# Start all services (main app + dependencies)
just docker-up

# Check service health
just docker-health
docker-compose ps

# View logs (all services)
just docker-logs

# View logs (specific service)
docker-compose logs -f myproject

# Execute command in running container
docker-compose exec myproject bash

# Restart specific service
docker-compose restart myproject

# Stop all services
just docker-down
```

4. **Volume management (3-tier strategy)**:
```bash
# Tier 1: Configs (read-only hot-reload)
# - Mount: ./configs:/app/configs:ro
# - Use: Configuration files that can be updated without rebuild
# - Example: YAML configs, JSON settings

# Tier 2: Ephemeral (session data)
# - Mount: ./ephemeral:/app/ephemeral
# - Use: Temporary data that survives restarts
# - Example: Cache, session storage

# Tier 3: Persistent (long-term data)
# - Mount: ./logs:/app/logs, ./data:/app/data, ./.chora/memory:/app/.chora/memory
# - Use: Critical data that must survive container removal
# - Example: Logs, database, event memory (SAP-010)

# Validate volume mounts
docker-compose config
docker inspect myproject | grep -A 20 Mounts
```

5. **Optimize Docker build context**:
```bash
# Check current build context size
du -sh .

# Review .dockerignore patterns
cat .dockerignore

# Expected exclusions (81% reduction):
# - .git/ (largest contributor)
# - tests/, docs/ (not needed in production)
# - .venv/, __pycache__/ (Python artifacts)
# - *.md (documentation)
# - .github/ (CI configs)

# Verify build context reduction
docker build --no-cache -t myproject:latest . 2>&1 | grep "Sending build context"
```

6. **Debug Docker containers**:
```bash
# Check container logs
docker-compose logs -f myproject

# Execute shell in running container
docker-compose exec myproject bash

# Inspect container configuration
docker inspect myproject

# Check resource usage
docker stats myproject

# View health check status
docker inspect myproject | grep -A 10 Health

# Rebuild from scratch (no cache)
docker build --no-cache -t myproject:latest .
```

**Integration with Other SAPs**:
- **SAP-005 (CI/CD)**: Dockerfile.test powers GitHub Actions test workflows with Docker cache (6x speedup)
- **SAP-010 (Memory System)**: Docker volume mounts for .chora/memory persistence across container restarts
- **SAP-014 (MCP Server)**: MCP servers deployed as Docker containers with health checks
- **SAP-008 (Automation)**: Docker build/run scripts integrated with justfile recipes

**Troubleshooting**:

| Issue | Diagnostic | Fix |
|-------|-----------|-----|
| Large image size | `docker images` | Check .dockerignore, use multi-stage build |
| Slow builds | `docker build --progress=plain` | Enable GitHub Actions cache, optimize layers |
| Import errors | `docker run --rm myproject:latest python -c "import mypackage"` | Use wheel distribution (not editable install) |
| Health check failing | `docker inspect myproject \| grep Health` | Verify entrypoint, check logs |
| Volume mount issues | `docker inspect myproject \| grep Mounts` | Check paths, permissions, read-only flags |

**L1 Achievement Evidence**:
- âœ… Dockerfile (multi-stage production build, 150-250MB)
- âœ… Dockerfile.test (CI-optimized, 6x faster with cache)
- âœ… docker-compose.yml (orchestration with volumes, networks, health checks)
- âœ… .dockerignore (81% build context reduction)
- âœ… justfile recipes (8 Docker commands for build, test, compose)

**ROI Metrics**:
- **Image size**: 40% smaller (150-250MB vs 500MB+ with editable install)
- **CI builds**: 6x faster (3 min â†’ 30 sec with GitHub Actions cache)
- **Reproducibility**: 100% (identical environments across dev, staging, production)
- **Deployment**: 5 min setup with docker-compose (vs 30+ min manual setup)

---

### SAP Framework (Capability Packaging Standard) - SAP-000 L1

**Purpose**: Define the Skilled Awareness Package framework with 5 standardized artifacts, versioning, installation blueprints, and governance for packaging capabilities as modular, discoverable units.

**Adoption Level**: L1 (Foundational - used by all 32+ SAPs)

**Core SAP Framework**:
```bash
# 5 standardized artifacts for every SAP:
docs/skilled-awareness/<capability-name>/
â”œâ”€â”€ capability-charter.md       # Problem, solution, scope, stakeholders, lifecycle
â”œâ”€â”€ protocol-spec.md            # Technical contract, interfaces, data models
â”œâ”€â”€ awareness-guide.md          # Agent patterns, workflows, troubleshooting
â”œâ”€â”€ adoption-blueprint.md       # Installation steps, validation, upgrade paths
â””â”€â”€ ledger.md                   # Adopter registry, version history, deployments

# YAML frontmatter (all artifacts):
---
sap_id: SAP-NNN
version: X.Y.Z                  # Semantic versioning
status: Draft|Pilot|Active|Deprecated|Archived
last_updated: YYYY-MM-DD
---
```

**Session Startup Routine** (agents should execute):
```bash
# 1. List all available SAPs
just list-saps                          # Show all 32+ SAPs from catalog

# 2. Validate existing SAP structure
just validate-all-saps                  # Check all SAPs in docs/skilled-awareness/

# 3. Generate new SAP (if creating capability)
just generate-sap SAP-042               # Scaffold from catalog metadata
just validate-sap-structure docs/skilled-awareness/my-capability/
```

**Common Workflows**:

1. **Create new SAP from scratch**:
```bash
# 1. Add SAP metadata to sap-catalog.json
cat >> sap-catalog.json <<EOF
{
  "id": "SAP-042",
  "name": "my-capability",
  "full_name": "My Capability",
  "status": "draft",
  "version": "0.1.0",
  "description": "Brief description",
  "capabilities": ["Feature 1", "Feature 2"],
  "dependencies": ["SAP-000"],
  "tags": ["category"]
}
EOF

# 2. Generate SAP artifacts from catalog
just generate-sap SAP-042

# 3. Validate generated structure
just validate-sap-structure docs/skilled-awareness/my-capability/

# 4. Fill in each artifact (using templates)
# - capability-charter.md: Problem statement, solution design
# - protocol-spec.md: Technical contract, interfaces
# - awareness-guide.md: Agent execution patterns
# - adoption-blueprint.md: Installation steps
# - ledger.md: Adopter registry (initially empty)

# 5. Validate completed SAP
just validate-sap SAP-042
```

2. **Validate SAP structure (5 artifacts + frontmatter)**:
```bash
# Validate specific SAP
just validate-sap-structure docs/skilled-awareness/my-capability/

# Validate all SAPs
just validate-all-saps

# Check frontmatter schema
grep -A 5 "^---$" docs/skilled-awareness/my-capability/protocol-spec.md

# Expected output:
# ---
# sap_id: SAP-042
# version: 0.1.0
# status: draft
# last_updated: 2025-11-09
# ---
```

3. **Check SAP maturity (quick evaluation)**:
```bash
# Evaluate SAP readiness
just validate-sap SAP-042

# Maturity levels:
# - Draft: Basic structure complete (5 artifacts exist)
# - Pilot: Dogfooding in progress (1+ adopter in ledger)
# - Active: Production-ready (3+ adopters, documented upgrade path)
# - Deprecated: Migration path available
# - Archived: Historical reference only
```

4. **Update SAP version (semantic versioning)**:
```bash
# Update version in all 5 frontmatters
# Major (breaking): 1.0.0 â†’ 2.0.0
# Minor (features): 1.0.0 â†’ 1.1.0
# Patch (fixes): 1.0.0 â†’ 1.0.1

# Example: Bump to 1.1.0
sed -i 's/version: 1.0.0/version: 1.1.0/' docs/skilled-awareness/my-capability/*.md

# Update last_updated date
sed -i "s/last_updated: .*/last_updated: $(date +%Y-%m-%d)/" docs/skilled-awareness/my-capability/*.md

# Document changes in ledger.md
cat >> docs/skilled-awareness/my-capability/ledger.md <<EOF
| 1.1.0 | $(date +%Y-%m-%d) | Added feature X, fixed bug Y |
EOF
```

5. **Track SAP adoption (ledger registry)**:
```bash
# Add adopter to ledger.md
cat >> docs/skilled-awareness/my-capability/ledger.md <<EOF
| project-name | 1.0.0 | active | $(date +%Y-%m-%d) |
EOF

# View all adopters
grep "| project-" docs/skilled-awareness/my-capability/ledger.md

# Count active deployments
grep "| active |" docs/skilled-awareness/my-capability/ledger.md | wc -l
```

6. **Validate prerequisites before SAP installation**:
```bash
# Check Python version, Git, dependencies
just validate-prerequisites

# Verify dependency SAPs are installed
grep "dependencies.*SAP-" docs/skilled-awareness/my-capability/capability-charter.md

# Example: SAP-042 depends on SAP-000, SAP-004
# Verify SAP-000 is installed: ls docs/skilled-awareness/sap-framework/
# Verify SAP-004 is installed: ls docs/skilled-awareness/testing-framework/
```

**Integration with Other SAPs**:
- **SAP-029 (SAP Generation)**: Automate SAP artifact creation from templates
- **SAP-027 (Dogfooding)**: Validate SAP adoption in real projects before marking "Active"
- **SAP-009 (Agent Awareness)**: Nested AGENTS.md/CLAUDE.md pattern used by all SAPs
- **All SAPs (32+)**: SAP-000 is the foundation - every SAP follows this framework

**Troubleshooting**:

| Issue | Diagnostic | Fix |
|-------|-----------|-----|
| Missing artifacts | `ls docs/skilled-awareness/my-capability/` | Run `just generate-sap SAP-042` |
| Invalid frontmatter | `grep -A 5 "^---$" *.md` | Fix YAML syntax, ensure sap_id/version/status/last_updated |
| Validation fails | `just validate-sap-structure path/` | Check artifact names match required filenames exactly |
| Generate fails | `cat sap-catalog.json \| grep SAP-042` | Ensure SAP exists in catalog with valid metadata |
| Version mismatch | `grep "version:" *.md` | Sync all 5 frontmatters to same version |

**L1 Achievement Evidence**:
- âœ… 5 artifact schema defined (charter, protocol, awareness, blueprint, ledger)
- âœ… YAML frontmatter standard (sap_id, version, status, last_updated)
- âœ… Semantic versioning (Major.Minor.Patch with compatibility tracking)
- âœ… Generation tools (just generate-sap, templating)
- âœ… Validation tools (just validate-sap-structure, just validate-all-saps)
- âœ… 32+ SAPs using framework (100% adoption in chora-base)

**ROI Metrics**:
- **Documentation time**: 70-80% reduction (standardized artifacts vs ad-hoc docs)
- **Discovery improvement**: 90%+ (agents find capabilities via adoption blueprints)
- **Consistency**: 100% (all SAPs follow same structure, frontmatter, versioning)
- **Adoption tracking**: Ledger provides complete history (who, when, which version)

---

### Development Lifecycle (8-Phase Workflow) - SAP-012 L3

**Purpose**: Define an 8-phase development lifecycle integrating Documentation-Driven Development â†’ BDD â†’ TDD methodologies into a unified workflow that reduces defects by 40-80% while maintaining velocity.

**Adoption Level**: L3 (Documentation-First workflow with 10+ executable how-tos)

**Core Development Lifecycle**:
```bash
# 8-phase workflow: Vision (months) â†’ Monitoring (continuous)
Phase 1: Vision & Strategy (Months)        # Strategic roadmap, capability waves
Phase 2: Planning & Prioritization (Weeks) # Sprint planning, backlog grooming
Phase 3: Requirements & Design (Days)      # Documentation-Driven Development â†’ BDD
Phase 4: Development (BDD + TDD) (Days-Weeks) # Gherkin scenarios â†’ TDD cycles
Phase 5: Testing & Quality (Hours-Days)    # Unit â†’ Smoke â†’ Integration â†’ E2E
Phase 6: Review & Integration (Hours-Days) # Code review, CI/CD, merge
Phase 7: Release & Deployment (Hours)      # Version bump, publish PyPI
Phase 8: Monitoring & Feedback (Continuous) # Metrics, retrospectives

# Documentation-Driven Development â†’ BDD â†’ TDD integration:
Docs (Phase 3) â†’ BDD Scenarios (RED) â†’ TDD Cycles (RED-GREEN-REFACTOR) â†’ All GREEN
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Check current phase from project state
ls docs/project-docs/plans/sprint-*.md      # Phase 2: Active sprint?
ls features/*.feature                       # Phase 4: BDD scenarios in progress?
pytest tests/ --collect-only                # Phase 5: Tests ready?

# 2. Show lifecycle help and available commands
just lifecycle-help                         # Display 8 phases + commands

# 3. Validate quality gates (if in Phase 5+)
just quality-gates                          # Run coverage, linting, type checking
```

**Common Workflows**:

1. **Phase 2: Create sprint plan (Planning)**:
```bash
# Create new sprint plan from template
just create-sprint-plan $(date +%Y-%m-%d)

# Output: docs/project-docs/plans/sprint-YYYY-MM-DD.md
# Edit sprint plan with:
# - Sprint goal
# - Selected features (from backlog)
# - Task breakdown
# - Capacity estimate (â‰¤80%)
```

2. **Phase 3: Documentation-First requirements (L3 pattern)**:
```bash
# L2 Pattern: Write documentation manually, then BDD scenarios manually
# 1. Write how-to guide: docs/user-docs/how-to/feature-name.md
# 2. Manually create BDD scenarios from acceptance criteria

# L3 Pattern: Write executable how-to, extract BDD scenarios automatically
just doc-to-bdd docs/user-docs/how-to/user-authentication.md

# Output: features/user-authentication.feature (extracted from how-to)
# Benefits: Documentation = Executable specification
# ROI: 50% faster than writing BDD manually, 0% drift between docs and tests
```

3. **Phase 4: BDD â†’ TDD development workflow**:
```bash
# Step 1: Create BDD scenario (RED - all scenarios fail)
just bdd-scenario features/user-authentication.feature

# Edit scenario:
# Feature: User authentication
#   Scenario: User logs in successfully
#     Given a registered user with email "user@example.com"
#     When they submit valid credentials
#     Then they should see the dashboard
#     And their session should be active

# Step 2: Run BDD scenarios (confirm RED)
pytest features/ --gherkin

# Step 3: TDD cycle for each scenario step
just tdd-cycle tests/test_authentication.py

# TDD workflow:
# 1. Write test (RED): assert user.is_authenticated == True
# 2. Implement minimal code (GREEN): user.is_authenticated = True
# 3. Refactor: Improve design, tests stay GREEN
# 4. Repeat until all BDD scenario steps pass

# Step 4: Confirm BDD scenarios turn GREEN
pytest features/ --gherkin
# âœ… All scenarios should pass
```

4. **Phase 5: Run quality gates (Testing)**:
```bash
# Run all quality gates (coverage, linting, type checking, security)
just quality-gates

# Expected output:
# 1ï¸âƒ£ Unit tests with coverage... âœ… 87% coverage (â‰¥85% required)
# 2ï¸âƒ£ Linting with ruff... âœ… 0 errors
# 3ï¸âƒ£ Type checking with mypy... âœ… 0 errors
# 4ï¸âƒ£ Security checks... âœ… 0 vulnerabilities

# Run full test suite (Unit â†’ Smoke â†’ Integration â†’ E2E)
just test-all

# If quality gates fail:
# - Coverage <85%: Add unit tests for uncovered code
# - Linting errors: Run `ruff check . --fix` to auto-fix
# - Type errors: Add type hints to functions
# - Security issues: Fix or suppress with justification
```

5. **Phase 7: Release workflow (Deployment)**:
```bash
# Step 1: Bump version (semantic versioning)
just bump-version minor  # 1.2.0 â†’ 1.3.0 (new features)
# OR
just bump-version patch  # 1.2.0 â†’ 1.2.1 (bug fixes)
# OR
just bump-version major  # 1.2.0 â†’ 2.0.0 (breaking changes)

# Step 2: Prepare release (changelog, git tag, build)
just prepare-release
# - Updates CHANGELOG.md with commits since last release
# - Creates git tag (e.g., v1.3.0)
# - Builds Python package (wheel + source dist)

# Step 3: Publish to production
just publish-prod
# - Publishes to PyPI (pip install your-package)
# - Deploys to production environment
# - Updates deployment documentation
```

6. **Phase 8: Monitoring and retrospectives (Continuous)**:
```bash
# Check process metrics
cat docs/project-docs/PROCESS_METRICS.md

# Review sprint retrospective
# - What went well?
# - What didn't go well?
# - Action items for next sprint

# Update strategic roadmap based on feedback
vim docs/project-docs/ROADMAP.md
```

**Light+ Planning Constructs** (4-level hierarchy):
```bash
# Construct 1: Strategy (Quarterly) - 3-6 month vision
docs/project-docs/ROADMAP.md
docs/project-docs/vision/

# Construct 2: Releases (Sprint-based) - 1-2 week milestones
docs/project-docs/plans/sprint-YYYY-MM-DD.md

# Construct 3: Features (User capabilities) - Days to weeks
docs/project-docs/features/feature-name/
- DDD worksheets
- BDD scenarios
- Acceptance criteria

# Construct 4: Tasks (Work items) - 2-8 hour chunks
.beads/issues.jsonl                 # SAP-015 integration
bd ready --json                     # Find unblocked tasks
bd show {id} --json                 # Task details

# Planning flows:
# Quarterly: Review metrics â†’ Define themes â†’ Update ROADMAP
# Sprint: Review themes â†’ Select features â†’ Create sprint goal â†’ Break into tasks
# Feature: Define value â†’ DDD workflow â†’ Break into tasks â†’ Add to sprint
# Daily: Pick task â†’ Execute BDD/TDD â†’ Update status â†’ Log progress
```

**Integration with Other SAPs**:
- **SAP-015 (Task Tracking)**: Tasks (Construct 4) managed via Beads CLI
- **SAP-010 (A-MEM)**: Event-sourced memory tracks development history across phases
- **SAP-005 (CI/CD)**: Automate Phase 5 (Testing), Phase 6 (Review), Phase 7 (Release)
- **SAP-027 (Dogfooding)**: Validate lifecycle phases through real project adoption
- **SAP-009 (Agent Awareness)**: AGENTS.md/CLAUDE.md pattern for phase-specific guidance
- **SAP-004 (Testing)**: pytest framework used in Phase 5 quality gates

**Troubleshooting**:

| Issue | Diagnosis | Fix |
|-------|-----------|-----|
| BDD scenarios failing (RED) after implementation | TDD cycles incomplete | Continue TDD red-green-refactor until scenarios GREEN |
| Quality gates failing (coverage <85%) | Insufficient unit tests | Add tests for uncovered code paths |
| Documentation out of sync with implementation | Skipped Phase 3 (Documentation-First) | Update docs BEFORE implementation next time |
| Features drift from strategic themes | Weak Phase 1 â†’ Phase 2 linkage | Explicitly link each feature to strategic theme in sprint plan |
| Tasks blocked frequently | Inadequate dependency tracking | Use `.beads/issues.jsonl` with dependency fields (SAP-015) |

**L3 Achievement Evidence**:
- âœ… **10+ executable how-tos**: docs/user-docs/how-to/ directory with actionable guides
- âœ… **BDD extraction workflow**: `just doc-to-bdd` command extracts scenarios from how-tos
- âœ… **Automated quality gates**: `just quality-gates` enforces â‰¥85% coverage + linting + types
- âœ… **Release automation**: `just bump-version`, `just prepare-release`, `just publish-prod` workflows
- âœ… **Planning hierarchy integrated**: Strategy â†’ Releases â†’ Features â†’ Tasks with traceability
- âœ… **Documentation-First validated**: 0% drift between how-tos and BDD scenarios (automated extraction)

**ROI Metrics**:
- **Defect reduction**: 40-80% (research-backed, documentation-driven + BDD + TDD integration)
- **Debugging time**: 60% reduction (shift-left testing catches defects in Phase 5, not Phase 8)
- **Traceability**: 100% (every task links to feature â†’ release â†’ strategy)
- **Documentation accuracy**: 100% (L3 pattern: how-tos = executable specifications)
- **Release reliability**: 95%+ (automated quality gates prevent defective releases)

---

### Testing Framework (pytest) - SAP-004 L3

**Purpose**: Provide automated testing with pytest framework, 85%+ coverage enforcement, and rich test patterns (parametrized, fixtures, mocks).

**Adoption Level**: L3 (Fully integrated, production-ready)

**Core Test Framework**:
- **pytest**: Industry-standard Python testing framework
- **Coverage gate**: 85%+ required (enforced via pytest.ini fail_under=85)
- **Test categories**: Unit (70%), Integration (20%), E2E (10%)
- **Performance**: <60s full suite, <5s unit tests
- **Quality**: <5% flakiness, 100% pass rate required for merge

**Current Metrics** (2025-11-09):
- **Test coverage**: 85.00% (achieved L3 target)
- **Test count**: 60 tests (42 unit, 12 integration, 6 E2E)
- **Execution time**: 0.52s (full suite)
- **Pass rate**: 100% (0 failures)

**Test Patterns in Use**:
```bash
# Basic tests: 60/60 (100%)
def test_basic_function():
    assert function() == expected

# Parametrized tests: ~35/60 (~58%)
@pytest.mark.parametrize("input,expected", [(1, 2), (2, 4)])
def test_with_params(input, expected):
    assert function(input) == expected

# Fixtures: ~25/60 (~42%)
@pytest.fixture
def sample_data():
    return {"key": "value"}

def test_with_fixture(sample_data):
    assert sample_data["key"] == "value"

# Mocks: ~18/60 (~30%)
def test_with_mock(mocker):
    mock_api = mocker.patch('module.api_call')
    mock_api.return_value = {"status": "ok"}
    assert function() == expected

# Error handling: ~48/60 (~80%)
def test_error_handling():
    with pytest.raises(ValueError):
        function_that_raises()
```

**Session Startup Routine** (agents should execute):
```bash
# 1. Run tests to verify environment
pytest --cov=src --cov-report=term --cov-fail-under=85
just test

# 2. If tests fail, investigate
pytest -vv --tb=short                    # Verbose output
pytest --lf                              # Re-run last failures only

# 3. If coverage below 85%, identify gaps
coverage report --show-missing           # Show uncovered lines
coverage html && open htmlcov/index.html # Visual coverage report
```

**Test Workflows** (by category):
```bash
# Unit tests (fast, <5s)
pytest -m unit -v
just test-unit

# Integration tests (moderate, ~2s)
pytest -m integration -v
just test-integration

# Specific test file
pytest tests/test_install_sap.py -v
just test-file tests/test_install_sap.py

# Pattern-based testing
pytest -k "test_sap" -v                  # Tests matching "test_sap"
pytest tests/ -k "not slow" -v           # Exclude slow tests
```

**Writing New Tests** (agents should follow):
```bash
# 1. Create test file: tests/test_<module>.py
touch tests/test_new_feature.py

# 2. Add test function with descriptive name
def test_new_feature_handles_empty_input():
    result = new_feature(input=[])
    assert result == []

# 3. Use parametrize for multiple cases
@pytest.mark.parametrize("input,expected", [
    ([], []),
    ([1], [1]),
    ([1, 2], [1, 2]),
])
def test_new_feature_with_params(input, expected):
    assert new_feature(input) == expected

# 4. Run tests to verify
pytest tests/test_new_feature.py -v

# 5. Check coverage
pytest --cov=src/module --cov-report=term
```

**Coverage Breakdown** (current state):
- **scripts/install-sap.py**: 79% (target: 85%, improvement needed)
- **scripts/usage_tracker.py**: 17% (low-priority, skip for now)
- **tests/conftest.py**: 94% (excellent)
- **tests/test_install_sap.py**: 100% (excellent, 60 tests)

**Integration with Other SAPs**:
- **SAP-005 (CI/CD)**: Automated test execution via test.yml workflow (Python 3.11, 3.12, 3.13 matrix)
- **SAP-006 (Quality Gates)**: Pre-commit hooks run pytest on staged files
- **SAP-031 (Enforcement)**: Testing as Layer 1 enforcement (70% prevention via TDD)
- **SAP-015 (Task Tracking)**: Test failures â†’ Create beads tasks for tracking fixes
- **SAP-009 (Awareness)**: Document test patterns in tests/AGENTS.md (if exists)

**Troubleshooting Test Failures**:

1. **Coverage below 85%**:
   ```bash
   # Identify uncovered lines
   coverage report --show-missing

   # Add tests for uncovered code
   # Focus on high-value paths first
   ```

2. **Flaky tests**:
   ```bash
   # Run test multiple times
   pytest --count=10 tests/test_flaky.py

   # Add proper waits, mocks, or fixtures
   # Avoid time-based assertions
   ```

3. **Slow tests**:
   ```bash
   # Mark slow tests
   @pytest.mark.slow
   def test_expensive_operation():
       ...

   # Run without slow tests
   pytest -m "not slow"
   ```

**L3 Achievement Evidence** (2025-11-09):
- âœ… 85%+ coverage achieved (fail_under=85 in pytest.ini)
- âœ… 60 tests (42 unit, 12 integration, 6 E2E)
- âœ… Fast execution (<60s full suite, <5s unit tests)
- âœ… CI integration (test.yml workflow, Python 3.11-3.13 matrix)
- âœ… Test patterns documented (parametrize 58%, fixtures 42%, mocks 30%)
- âœ… Justfile recipes (6 test commands)
- âœ… Quality gates (100% pass rate required for merge)

**ROI Metrics**:
- **Bug prevention**: 90% via TDD (catch bugs before code review)
- **Time savings**: 15-20 min per session (avoid manual testing)
- **Regression prevention**: 95%+ (automated regression testing on every PR)
- **Refactoring confidence**: 100% (tests validate behavior preservation)

**Documentation**:
- Protocol specification: [docs/skilled-awareness/testing-framework/protocol-spec.md](docs/skilled-awareness/testing-framework/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/testing-framework/adoption-blueprint.md](docs/skilled-awareness/testing-framework/adoption-blueprint.md)
- Test patterns guide: [docs/skilled-awareness/testing-framework/awareness-guide.md](docs/skilled-awareness/testing-framework/awareness-guide.md)
- Domain awareness: [tests/AGENTS.md](tests/AGENTS.md) (if exists)

---

### Agent Awareness (Nested AGENTS.md/CLAUDE.md) - SAP-009 L3

**Purpose**: Provide structured agent guidance through nested AGENTS.md/CLAUDE.md files using "nearest file wins" pattern for progressive context loading.

**Adoption Level**: L3 (Universal pattern adopted across all domains)

**Core Pattern**: Dual-file hierarchy with domain-specific awareness
- **AGENTS.md**: Generic agent patterns (all agents)
- **CLAUDE.md**: Claude-specific optimizations

**Nested Hierarchy (5 Levels)**:
```
/AGENTS.md, /CLAUDE.md                           â† Root (project overview)
â”œâ”€ tests/AGENTS.md                               â† Domain: Testing
â”œâ”€ scripts/AGENTS.md                             â† Domain: Scripts
â”œâ”€ .chora/AGENTS.md, .chora/CLAUDE.md            â† Domain: Memory (SAP-010)
â”œâ”€ docs/skilled-awareness/AGENTS.md              â† Domain: SAP capabilities
â”‚  â”œâ”€ inbox/AGENTS.md, inbox/CLAUDE.md           â† SAP-001 (Inbox)
â”‚  â”œâ”€ agent-awareness/AGENTS.md, CLAUDE.md       â† SAP-009 (this pattern)
â”‚  â”œâ”€ memory-system/AGENTS.md, CLAUDE.md         â† SAP-010 (Memory)
â”‚  â””â”€ ... (30+ SAPs with nested awareness)
```

**Progressive Loading Phases**:
1. **Phase 1 (Essential, 0-10k tokens)**:
   - Read root AGENTS.md sections 1-2 (project overview, development process)
   - Load only files directly related to current task
   - **Result**: Quick orientation, minimal token usage

2. **Phase 2 (Extended, 10-50k tokens)**:
   - Read root AGENTS.md fully + domain-specific AGENTS.md
   - Read CLAUDE.md for Claude optimizations
   - Load related SAP protocol-spec.md if implementing capability
   - **Result**: Full context for implementation, 60-70% token savings vs loading everything

3. **Phase 3 (Comprehensive, 50-200k tokens)**:
   - Read all SAP artifacts (capability-charter.md, ledger.md)
   - Load full project history and all documentation
   - **Used only for**: Complex refactoring, architectural decisions, deep debugging

**Domain-Specific Files** (60-70% token reduction):
- **tests/AGENTS.md** (~200 lines, 5-min read): Testing patterns, fixtures, coverage targets
- **scripts/AGENTS.md** (~250 lines, 6-min read): Cross-platform script patterns, validation
- **.chora/AGENTS.md** (~400 lines, 13-min read): Memory system workflows, event logging
- **docs/skilled-awareness/{SAP}/AGENTS.md**: SAP-specific patterns and examples

**Session Startup Routine** (agents should execute this):
```bash
# 1. Read root awareness (Phase 1)
cat AGENTS.md | head -100     # First 100 lines for quick orientation

# 2. Navigate to domain-specific awareness (Phase 2)
cat tests/AGENTS.md           # If working on tests (60% token savings)
cat scripts/AGENTS.md         # If working on scripts (65% token savings)
cat .chora/AGENTS.md          # If working with memory (70% token savings)

# 3. Read Claude optimizations
cat CLAUDE.md                 # Claude-specific patterns (artifact-first, checkpoints)

# 4. Validate awareness structure
just validate-awareness-structure AGENTS.md
just validate-awareness-links
```

**Validation Commands**:
```bash
# Validate AGENTS.md structure (7 required sections)
python scripts/validate-awareness-structure.py AGENTS.md

# Check for broken links in awareness network
python scripts/validate-awareness-links.py

# Show awareness hierarchy
just awareness-hierarchy

# Show awareness statistics
just awareness-stats
```

**Creating Domain-Specific Awareness**:
```bash
# Copy template for new domain
cp docs/skilled-awareness/templates/AGENTS.md.template my-domain/AGENTS.md
cp docs/skilled-awareness/templates/CLAUDE.md.template my-domain/CLAUDE.md

# Edit to add domain-specific patterns
vim my-domain/AGENTS.md

# Validate structure
just validate-awareness-structure my-domain/AGENTS.md
```

**Token Efficiency Tracking** (integrated with SAP-013):
```python
from utils.claude_metrics import ClaudeROICalculator, TokenUsageMetric
from datetime import datetime

calculator = ClaudeROICalculator(developer_hourly_rate=100.0)

# Track a session with progressive loading
metric = TokenUsageMetric(
    session_id="session-2025-11-09-001",
    timestamp=datetime.now(),
    tokens_used=35000,                    # Used Phase 2 (extended context)
    tokens_available=200000,
    progressive_loading_phase=2,          # Loaded domain-specific AGENTS.md
    context_items_loaded=5,               # Root + domain + 3 files
    task_completed=True,
    metadata={"task_type": "feature_implementation", "domain": "tests"}
)
calculator.track_token_usage(metric)

# Generate token usage report
print(calculator.generate_token_usage_report())
```

**Integration with Other SAPs**:
- **ALL SAPs**: Every SAP uses nested awareness pattern for discoverability
- **SAP-010 (Memory)**: Domain-specific .chora/AGENTS.md for memory workflows
- **SAP-015 (Task Tracking)**: Document beads patterns in AGENTS.md
- **SAP-001 (Inbox)**: Domain-specific inbox/AGENTS.md for coordination
- **SAP-027 (Dogfooding)**: Validate awareness adoption completeness

**L3 Achievement Evidence** (2025-11-09):
- âœ… Nested hierarchy implemented (5 levels): root â†’ domain â†’ SAP â†’ feature â†’ component
- âœ… Domain-specific files created: tests/, scripts/, .chora/, inbox/, agent-awareness/
- âœ… Progressive loading documented in CLAUDE.md with 3 phases
- âœ… Token tracking integrated with SAP-013 metrics framework
- âœ… Validation scripts available: validate-awareness-structure.py, validate-awareness-links.py
- âœ… Justfile recipes: 7 awareness commands (validate, hierarchy, stats, create)
- âœ… Template files for creating new domain awareness: AGENTS.md.template, CLAUDE.md.template

**ROI Metrics**:
- **Token reduction**: 60-70% via domain-specific files (Phase 2 vs Phase 3)
- **Onboarding time**: 5-10 min faster per session (targeted reading vs full codebase scan)
- **Context restoration**: <2 min via "nearest file wins" pattern
- **Target**: <50k tokens average per session (current: ~35k baseline)
- **Progressive loading adoption**: â‰¥90% sessions using Phase 1-2 (vs Phase 3 full load)

**Documentation**:
- Protocol specification: [docs/skilled-awareness/agent-awareness/protocol-spec.md](docs/skilled-awareness/agent-awareness/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/agent-awareness/adoption-blueprint.md](docs/skilled-awareness/agent-awareness/adoption-blueprint.md)
- Domain-specific guide: [docs/skilled-awareness/agent-awareness/AGENTS.md](docs/skilled-awareness/agent-awareness/AGENTS.md)
- Claude patterns: [docs/skilled-awareness/agent-awareness/CLAUDE.md](docs/skilled-awareness/agent-awareness/CLAUDE.md)

**Troubleshooting**:
- **Broken links**: Run `just validate-awareness-links` to find and fix
- **Missing sections**: Run `just validate-awareness-structure FILE` to check 7 required sections
- **Duplicate content**: Keep root AGENTS.md generic, move specifics to domain files
- **Token overuse**: Check progressive loading phase, use domain-specific files for 60-70% reduction
- Phase 1 adoption: â‰¥90% of sessions
- Task completion rate: â‰¥95% (maintain current performance)

---

### CI/CD Workflows (GitHub Actions) - SAP-005 L3

**Purpose**: Provide automated testing, linting, security scanning, and release workflows using GitHub Actions for continuous integration and deployment.

**Adoption Level**: L3 (Fully automated, production-ready)

**Core Workflows** (.github/workflows/):

1. **test.yml** - Matrix testing across Python 3.11, 3.12, 3.13
   - Trigger: push (main, develop), pull_request
   - Coverage gate: 85%+ required
   - Duration: ~2-3 minutes
   - Status: âœ… Required for merge

2. **lint.yml** - Code quality gates (ruff + mypy)
   - Trigger: push (main, develop), pull_request
   - Checks: ruff (linting), mypy (type checking)
   - Duration: ~1-2 minutes
   - Status: âœ… Required for merge

3. **smoke.yml** - Quick validation (<1 min)
   - Trigger: push (main, develop), pull_request
   - Purpose: Server starts, basic operations
   - Duration: ~30-60 seconds
   - Status: âœ… Required for merge

4. **codeql.yml** - Security scanning
   - Trigger: push (main, develop), pull_request, schedule (weekly)
   - Purpose: Static security analysis, vulnerability detection
   - Duration: ~3-5 minutes
   - Status: âœ… Required for merge (security critical)

5. **dependency-review.yml** - Dependency security
   - Trigger: pull_request
   - Purpose: Check new dependencies for vulnerabilities
   - Duration: ~30-60 seconds
   - Status: âœ… Required for merge (security critical)

6. **release.yml** - PyPI publishing
   - Trigger: push (tags: 'v*'), workflow_dispatch
   - Purpose: Build wheel, publish to PyPI (OIDC trusted publishing)
   - Duration: ~2-3 minutes
   - Status: N/A (only for releases)

7. **cross-platform-test.yml** - Multi-OS validation
   - Trigger: push (main, develop), pull_request
   - Matrix: Ubuntu, Windows, macOS
   - Purpose: Cross-platform compatibility testing
   - Duration: ~5-7 minutes
   - Status: âœ… Required for merge

**CI Workflow Patterns** (agent should follow):

```bash
# 1. Check CI status at session startup
gh run list --limit 10
just ci-status

# 2. If CI failed, investigate logs
gh run view {run_id} --log
just ci-logs {run_id}

# 3. If transient failure, retry
gh run rerun {run_id}
just ci-retry {run_id}

# 4. If real failure, create bead to track fix (SAP-015)
bd create "Fix CI failure in test.yml" --priority high --blocker "ci-failure-{run_id}"

# 5. Fix locally first (pre-commit hooks via SAP-006)
pytest tests/               # Run tests locally
ruff check src/ tests/      # Run linting locally
mypy src/ tests/            # Run type checking locally

# 6. Push fix and verify CI passes
git push origin branch-name
gh run list --limit 5       # Verify new run passes
```

**Quality Gates** (enforced by CI):
- âœ… 85%+ test coverage (test.yml)
- âœ… Zero linting errors (lint.yml via ruff)
- âœ… Zero type errors (lint.yml via mypy)
- âœ… Zero security vulnerabilities (codeql.yml, dependency-review.yml)
- âœ… Cross-platform compatibility (cross-platform-test.yml)
- âœ… Smoke tests pass (smoke.yml)

**Integration with Other SAPs**:
- **SAP-004 (Testing)**: CI runs pytest test suites with 85%+ coverage gates
- **SAP-006 (Quality Gates)**: Pre-commit hooks (local) + CI workflows (remote) dual validation
- **SAP-028 (PyPI Publishing)**: OIDC trusted publishing in release.yml (zero long-lived tokens)
- **SAP-015 (Task Tracking)**: CI failure â†’ Create bead to track fix with blocker
- **SAP-031 (Enforcement)**: CI/CD as Layer 3 enforcement (9% prevention rate)

**Release Workflow** (agents should execute):

```bash
# 1. Bump version and create tag
python scripts/bump-version.py 1.2.3

# 2. Push tag to trigger release workflow
git push --tags

# 3. Monitor release workflow
gh run list --workflow=release.yml
gh run view {run_id} --log

# 4. Verify PyPI publication
# Test PyPI: https://test.pypi.org/project/{package_name}/
# Prod PyPI: https://pypi.org/project/{package_name}/

# 5. Create GitHub release from tag
gh release create v1.2.3 --notes-from-tag
```

**Troubleshooting CI Failures**:

1. **Coverage below 85%**:
   ```bash
   # Run coverage locally
   pytest --cov=src/package_name --cov-report=term --cov-report=html
   open htmlcov/index.html

   # Add missing tests for uncovered lines
   # Update tests/ directory
   ```

2. **Lint errors**:
   ```bash
   # Run ruff locally
   ruff check src/ tests/

   # Auto-fix fixable issues
   ruff check --fix src/ tests/
   ```

3. **Type errors**:
   ```bash
   # Run mypy locally
   mypy src/ tests/

   # Add type annotations or suppressions
   # Fix type mismatches
   ```

4. **Security vulnerabilities**:
   ```bash
   # Check CodeQL alerts
   gh api repos/{owner}/{repo}/code-scanning/alerts

   # Review dependency vulnerabilities
   gh api repos/{owner}/{repo}/dependabot/alerts

   # Update vulnerable dependencies
   pip install --upgrade {vulnerable_package}
   ```

5. **Cross-platform failures**:
   ```bash
   # Check which OS failed
   gh run view {run_id}

   # Read cross-platform guide
   cat scripts/AGENTS.md  # Cross-platform patterns

   # Use cross-platform template
   cp templates/cross-platform/python-script-template.py scripts/new-script.py
   ```

**L3 Achievement Evidence** (2025-11-09):
- âœ… 7 production workflows deployed (.github/workflows/)
- âœ… Matrix testing: Python 3.11, 3.12, 3.13 (Ubuntu, Windows, macOS)
- âœ… Quality gates: 85%+ coverage, ruff, mypy, CodeQL, dependency review
- âœ… Release automation: OIDC trusted publishing to PyPI
- âœ… Fast feedback: <5 min average workflow execution
- âœ… Justfile recipes: 6 CI commands (status, logs, retry, workflows, show, trigger)
- âœ… Integration with SAP-004, SAP-006, SAP-015, SAP-028, SAP-031

**ROI Metrics**:
- **Setup time**: 90% reduction (hours â†’ 5-10 minutes via pre-configured workflows)
- **Quality gates**: 95%+ preventable issues caught before merge
- **Feedback speed**: <5 min average workflow execution (cached dependencies, parallel jobs)
- **Security**: 100% automated scanning (CodeQL + dependency review)
- **Release time**: 80% reduction (manual publish â†’ one-command automated)

**Documentation**:
- Protocol specification: [docs/skilled-awareness/ci-cd-workflows/protocol-spec.md](docs/skilled-awareness/ci-cd-workflows/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/ci-cd-workflows/adoption-blueprint.md](docs/skilled-awareness/ci-cd-workflows/adoption-blueprint.md)
- Workflow details: [docs/skilled-awareness/ci-cd-workflows/awareness-guide.md](docs/skilled-awareness/ci-cd-workflows/awareness-guide.md)

---

## Common Tasks for AI Agents

### Task 1: Add New Optional Feature

**Goal**: Add a new opt-in feature to the template

**Steps**:
1. Update relevant templates in technology-specific SAPs (e.g., static-template/mcp-templates/ for MCP) to include optional feature toggles.
2. Add conditional logic or installation hooks in `static-template/` and supporting scripts.
3. Refresh SAP documents (especially protocol + adoption blueprint) to describe the new capability.
4. Update `template/AGENTS.md.jinja` if feature affects AI agent workflows.
5. Document in `README.md` under "Features" section.
6. Add to `CHANGELOG.md` under `## [Unreleased]`.
7. Test blueprint generation with feature enabled/disabled via `python setup.py`.

### Task 2: Fix Bug in Template

**Goal**: Correct error in template that affects generated projects

**Steps**:
1. Reproduce in generated project:
   ```bash
   python setup.py /tmp/bug-test
   cd /tmp/bug-test
   # Reproduce issue
   ```

2. Fix in `template/` source files

3. Test fix:
   ```bash
   python setup.py /tmp/bug-fix-test
   cd /tmp/bug-fix-test
   # Verify fix works
   ```

4. Update `CHANGELOG.md` under `## [Unreleased]` in "Fixed" section

5. Commit with `fix(scope): description`

6. Create PATCH release if critical bug

### Task 3: Release New Version

**See**: "Releasing New Version" workflow above

**Checklist**:
- [ ] All tests pass (`python setup.py /tmp/test && cd /tmp/test && pytest`)
- [ ] CHANGELOG.md updated (Unreleased â†’ [X.Y.Z])
- [ ] README.md updated (Recent Updates section)
- [ ] Upgrade guide created (`docs/upgrades/vX.Y-to-vX.Z.md`)
- [ ] Upgrade guide added to `docs/upgrades/README.md` table
- [ ] Commit with version in message
- [ ] Git tag created (`git tag vX.Y.Z`)
- [ ] Pushed to origin with tags (`git push origin main --tags`)
- [ ] GitHub release created (`gh release create vX.Y.Z`)

### Task 4: Update Research Documentation

**Goal**: Incorporate new research findings into template

**Steps**:
1. Save research document to `docs/research/`

2. Analyze research for actionable enhancements

3. Create plan with specific changes (e.g., Phase 1, 2, 3)

4. Implement changes (see Task 1: Add New Optional Feature)

5. Reference research in commit messages and upgrade guides

6. Update `README.md` or relevant docs with research-backed rationale

**Example**: v1.9.3 added super-tests, memory architecture, and query patterns based on "Agentic Coding Best Practices Research.pdf"

---

## Architecture Decisions

### Why Jinja2 Templating?

**Rationale**: Blueprints use Jinja2 placeholders for flexible customization and remain compatible with existing project content
**Trade-off**: Complex conditionals require disciplined structure; mitigate with helper scripts and clear template snippets

### Why Opt-In for Advanced Features?

**Rationale**: Prevent overwhelming new adopters, let users choose complexity
**Examples**: `documentation_advanced_features=false`, `include_docker=false`, `include_memory_system=true`

### Why Separate AGENTS.md.jinja (2,540 lines)?

**Known Issue**: AGENTS.md.jinja has grown 176% from v1.0.0 (782 lines) to v1.9.3 (2,540 lines) with zero refactoring
**Upcoming Change**: v2.0.0 will refactor to nested AGENTS.md structure per research recommendations
**Pattern**: Nearest file wins (tests/AGENTS.md, .chora/memory/AGENTS.md, etc.)
**Timeline**: v1.9.3 is final release before v2.0.0 refactoring

### Why docs/upgrades/ for Upgrade Guides?

**Rationale**: Centralized upgrade documentation, easy discovery, consistent structure
**Previous Mistake**: v1.9.0-to-v1.9.1 and v1.9.1-to-v1.9.2 were placed in repo root (lacked this AGENTS.md)
**Fix**: Moved to `docs/upgrades/` in v1.9.3 release

---

## Common Pitfalls & Prevention

### Pitfall 1: Misplaced Files

**Problem**: Placing files in wrong location (e.g., upgrade docs in repo root)
**Prevention**: Follow "File Organization Conventions" section above
**Detection**: Review `git status` before commit, check against conventions

### Pitfall 2: Forgetting Template Metadata

**Problem**: Creating blueprint files without required metadata or helper comments
**Prevention**: Follow blueprint conventions (header comments, clear placeholder instructions)
**Detection**: Run blueprint lint scripts or manual review before committing

### Pitfall 3: Breaking Conditional Logic

**Problem**: Adding `{% if ... %}` without updating supporting install scripts or awareness docs
**Prevention**: Test with feature on AND off via `python setup.py`; update SAP adoption blueprint accordingly
**Detection**: Generate project with feature disabled, verify files excluded and docs consistent

### Pitfall 4: Stale Examples

**Problem**: Example projects lag behind template changes
**Prevention**: Document in commit message: "Note: examples/ not updated (drift expected)"
**Detection**: Periodically regenerate examples using `python setup.py` and compare to current blueprint outputs

### Pitfall 5: Incomplete Upgrade Guides

**Problem**: Missing sections in upgrade guide (no rollback, no testing)
**Prevention**: Use `docs/upgrades/UPGRADE_GUIDE_TEMPLATE.md` as checklist
**Detection**: Review against template before committing

### Pitfall 6: Version Number Errors

**Problem**: Tagging wrong version or skipping version in CHANGELOG
**Prevention**: Update CHANGELOG first, then tag with matching version
**Detection**: `git log --oneline` and `git tag -l` should align

---

## Integration Points

### Blueprint Engine

**Tooling**: `setup.py` interactive generator (can be wrapped by automation scripts)
**Key Files**: `static-template/`, technology-specific SAP templates, `docs/skilled-awareness/`

### Generated Projects

**Update Mechanism**: Follow capability-specific SAP adoption blueprints and ledger guidance
**Configuration**: Project metadata stored in generated files (`pyproject.toml`, `AGENTS.md`, etc.); no `.copier-answers.yml`
**Merge Strategy**: Manual or scripted merge guided by SAP protocols (e.g., apply blueprint diffs, run tests, update docs)

### GitHub Actions

**Workflows**: Template includes 7+ CI/CD workflows in `template/.github/workflows/`
**Secrets Required**: `PYPI_TOKEN` (if PyPI publishing enabled)
**Dependabot**: Auto-merge workflow for dependency updates

### Docker

**Optional Feature**: `include_docker=true`
**Strategies**: `production` (multi-stage + compose) or `ci-only` (Dockerfile.test only)
**Integration**: justfile commands (`docker-build`, `docker-test`, etc.)

---

## Memory System (A-MEM) - SAP-010

**When to use SAP-010**:
- Capturing learnings, insights, or patterns discovered during work for reuse in future sessions
- Querying event logs to restore context after breaks (hours, days, or weeks between sessions)
- Logging significant events (milestones, decisions, errors) for audit trails
- Building knowledge graph with wikilink connections to relate discoveries
- Tracking agent behavior patterns and learned approaches across sessions

**Quick-start approach** (recommended):
```bash
# Work in memory system domain
cd .chora/

# Read domain-specific guidance (60-70% token savings vs root files)
cat AGENTS.md        # Generic memory patterns (13-min read)
cat CLAUDE.md        # Claude-specific workflows (8-min read)

# Log an event
echo '{"event_type":"learning_captured","timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'","data":{"pattern":"test-pattern"}}' >> memory/events/development.jsonl

# Create knowledge note
cp memory/knowledge/templates/default.md memory/knowledge/notes/my-pattern.md

# Query event logs
tail -n 20 memory/events/*.jsonl

# Check system health
python ../scripts/memory-health-check.py
```

**What you get**:
- **Event logging**: JSONL-format logs with trace correlation (CHORA_TRACE_ID), structured metadata, microsecond timestamps
- **Knowledge notes**: Markdown with YAML frontmatter, Zettelkasten wikilinks (`[[note-name]]`), confidence ratings (0.0-1.0)
- **Agent profiles**: YAML files capturing learned patterns, preferences, historical behavior
- **Query templates**: Reusable queries in `.chora/memory/queries/` for common analysis patterns
- **Nested awareness**: Domain-specific [.chora/AGENTS.md](.chora/AGENTS.md) and [.chora/CLAUDE.md](.chora/CLAUDE.md) for progressive context loading

**Example workflow**:
```bash
# Scenario: Capture learning from completed task integration

# 1. Complete task (SAP-015 beads)
bd close task-123 --reason "Implemented async error handling pattern"

# 2. Extract pattern â†’ knowledge note
cp .chora/memory/knowledge/templates/default.md .chora/memory/knowledge/notes/async-error-handling.md
# Edit note: Document pattern, add wikilinks to related notes

# 3. Log learning event
echo '{"event_type":"learning_captured","timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'","data":{"task_id":"task-123","pattern":"async-error-handling","confidence":0.9}}' >> .chora/memory/events/development.jsonl

# 4. Link to related knowledge
# In note, add: [[error-handling-patterns]] [[async-best-practices]]

# 5. Later: Query for context restoration (new session)
grep "async-error" .chora/memory/events/development.jsonl
ls -lt .chora/memory/knowledge/notes/*.md | head -10

# Result: Pattern available for future tasks, context restored in <2 minutes
```

**Nested awareness guides**:
- [.chora/AGENTS.md](.chora/AGENTS.md) - Memory system patterns (13-min read, ~10k tokens)
- [.chora/CLAUDE.md](.chora/CLAUDE.md) - Claude workflows (8-min read, ~5k tokens)
- **Progressive loading**: Load only what you need (60-70% token savings vs loading full root AGENTS.md)

**Integration with other SAPs**:
- **SAP-001 (Inbox)**: Coordination request received â†’ Log event in `events/inbox.jsonl`
- **SAP-015 (Task Tracking)**: Task completed â†’ Create knowledge note with learnings
- **SAP-012 (Planning)**: Sprint retrospective â†’ Distill insights to knowledge graph
- **SAP-009 (Awareness)**: Uses nested AGENTS.md/CLAUDE.md pattern for domain-specific guidance

**Documentation**:
- Domain guides: [.chora/AGENTS.md](.chora/AGENTS.md), [.chora/CLAUDE.md](.chora/CLAUDE.md)
- Protocol specification: [docs/skilled-awareness/memory-system/protocol-spec.md](docs/skilled-awareness/memory-system/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/memory-system/adoption-blueprint.md](docs/skilled-awareness/memory-system/adoption-blueprint.md)

**ROI**: 5-15 minutes saved per session via context restoration, 40-48 hours saved annually for active agents

**Note**: chora-base template repository itself does not use memory system (no `.chora/` directory in template repo). Memory system is included in generated projects when `include_memory_system=true`.

---

## Task Tracking (Beads) - SAP-015

**Status**: Pilot (v1.0.0) | **Adoption Level**: L0 (Available for installation)

SAP-015 provides persistent task tracking using `.beads/issues.jsonl` for cross-session context restoration. Eliminates work loss between Claude Code sessions by maintaining git-committed task state.

**Quick-start approach**:
```bash
# Domain-specific awareness files (if SAP-015 adopted in your project)
cd .beads/
cat AGENTS.md  # 10-min read: Beads workflow patterns
cat CLAUDE.md  # 7-min read: Claude-specific beads usage

# Alternative: Read full protocol specification
cat docs/skilled-awareness/task-tracking/protocol-spec.md  # 25-min read
```

**When to use SAP-015**:
- **Session startup**: Restore context from previous session (<2 min vs 5-10 min manual)
- **Multi-session work**: Track progress across multiple Claude Code sessions
- **Backlog management**: Prioritize tasks with dependencies and blockers
- **Audit trails**: Document completion reasons and link artifacts
- **Team coordination**: Share task state with other agents or humans

**Core CLI commands**:
```bash
# Session startup: Find unblocked work
bd ready --json                              # Programmatic (Claude Code)
bd ready                                     # Human-readable

# Claim task
bd update task-123 --status in_progress --assignee "claude-code"

# Add notes during work
bd update task-123 --notes "Implemented async error handling, tests passing"

# Complete task
bd close task-123 --reason "Feature implemented, tested, and documented"

# Query by status
bd list --status open --json                 # Open backlog
bd list --status in_progress --json          # Active work
bd list --status blocked --json              # Blocked tasks
bd list --status closed --json --limit 10   # Recent completions

# Query by assignee
bd list --assignee "claude-code" --json      # All Claude Code tasks
```

**Example workflow (multi-session coordination)**:
```bash
# Session 1: Start feature work
bd create --title "Implement user authentication" \
  --description "Add JWT-based auth with refresh tokens" \
  --priority high \
  --tags "feature,auth,security"

bd update task-456 --status in_progress --assignee "claude-code"

# Mid-session: Discover blocker
bd update task-456 --status blocked \
  --blockers "Waiting for API key from DevOps team" \
  --notes "Auth flow implemented, needs production API key for testing"

# Session 2 (next day): Check ready tasks
bd ready --json
# Output: [] (task-456 still blocked)

bd list --status blocked --json
# Output: Shows task-456 with blocker reason

# Session 3 (API key received): Resume work
bd update task-456 --status in_progress \
  --notes "API key received, testing auth flow"

# Complete task
bd close task-456 \
  --reason "Auth implemented, tested with production API, docs updated" \
  --artifacts "src/auth.py,tests/test_auth.py,docs/authentication.md"
```

**Task data structure**:
```json
{
  "id": "task-456",
  "title": "Implement user authentication",
  "description": "Add JWT-based auth with refresh tokens",
  "status": "closed",
  "priority": "high",
  "assignee": "claude-code",
  "tags": ["feature", "auth", "security"],
  "created": "2025-11-09T10:30:00Z",
  "updated": "2025-11-09T16:45:00Z",
  "closed": "2025-11-09T16:45:00Z",
  "blockers": [],
  "dependencies": [],
  "notes": "API key received, testing auth flow",
  "completion_reason": "Auth implemented, tested with production API, docs updated",
  "artifacts": ["src/auth.py", "tests/test_auth.py", "docs/authentication.md"]
}
```

**Integration with other SAPs**:
- **SAP-001 (Inbox)**: Coordination request â†’ Decompose into beads tasks
  ```bash
  # Receive coordination request
  cat inbox/incoming/coordination/COORD-2025-042.md

  # Create tasks from coordination request
  bd create --title "Task 1 from COORD-2025-042" --tags "coordination,COORD-2025-042"
  bd create --title "Task 2 from COORD-2025-042" --tags "coordination,COORD-2025-042"
  ```

- **SAP-010 (Memory)**: Task completed â†’ Extract learnings to knowledge notes
  ```bash
  # Complete task
  bd close task-789 --reason "Implemented async error handling pattern"

  # Extract pattern to knowledge note
  cp .chora/memory/knowledge/templates/default.md \
     .chora/memory/knowledge/notes/async-error-handling.md

  # Log learning event
  echo '{"event_type":"learning_captured","task_id":"task-789"}' >> \
    .chora/memory/events/development.jsonl
  ```

- **SAP-005 (CI/CD)**: CI failure â†’ Create bead to track fix
  ```bash
  # CI failure detected
  bd create --title "Fix CI test failure in test_api.py" \
    --priority urgent \
    --tags "ci,bug,test-failure"
  ```

**Nested awareness guides**:
- [.beads/AGENTS.md](.beads/AGENTS.md) - Beads workflow patterns (10-min read, ~8k tokens)
- [.beads/CLAUDE.md](.beads/CLAUDE.md) - Claude beads workflows (7-min read, ~5k tokens)
- **Progressive loading**: Load only what you need (60-70% token savings)

**Documentation**:
- Protocol specification: [docs/skilled-awareness/task-tracking/protocol-spec.md](docs/skilled-awareness/task-tracking/protocol-spec.md)
- Adoption blueprint: [docs/skilled-awareness/task-tracking/adoption-blueprint.md](docs/skilled-awareness/task-tracking/adoption-blueprint.md)
- CLI reference: `bd --help` or see protocol-spec.md Section 3

**ROI**: 5-10 minutes saved per session via context restoration, 40-80 hours saved annually for active projects

**Note**: chora-base template repository does not use beads (no `.beads/` directory in template repo). Beads is included in generated projects when adopted.

---

## SAP Evaluation Workflow

### Purpose

SAP-019 (Self-Evaluation) enables you to assess SAP adoption depth, identify gaps, and generate improvement roadmaps.

### When to Evaluate

- **After installing new SAP**: Validate installation (`--quick`)
- **Sprint planning**: Generate roadmap for next sprint (`--strategic`)
- **User asks "How's our SAP adoption?"**: Quick status check
- **User asks "How can we improve SAP-X?"**: Deep dive analysis
- **Quarterly reviews**: Track progress over time

### Evaluation Modes

**Quick Check** (30 seconds):
```bash
# Check all installed SAPs
python scripts/sap-evaluator.py --quick

# Check specific SAP
python scripts/sap-evaluator.py --quick SAP-004
```

**Deep Dive** (5 minutes):
```bash
# Analyze specific SAP, save report
python scripts/sap-evaluator.py --deep SAP-004 --output docs/adoption-reports/SAP-004-assessment.md
```

**Strategic Analysis** (30 minutes):
```bash
# Generate quarterly roadmap
python scripts/sap-evaluator.py --strategic --output docs/adoption-reports/sap-roadmap.yaml
```

### Common Workflow: "How can we improve SAP-X?"

1. **Run deep dive**:
   ```bash
   python scripts/sap-evaluator.py --deep SAP-004 --output docs/adoption-reports/SAP-004-assessment.md
   ```

2. **Read report** (use Read tool):
   ```
   Read: docs/adoption-reports/SAP-004-assessment.md
   ```

3. **Extract top 3 gaps** from report:
   - Gap 1 (P0): [Title], [Impact], [Effort], [Actions]
   - Gap 2 (P1): [...]
   - Gap 3 (P2): [...]

4. **Present to user**:
   ```markdown
   ## SAP-004 Improvement Opportunities

   **Current Level**: 1 (Basic)
   **Next Milestone**: Level 2

   ### Priority Gaps
   1. **[Gap 1 Title]** (P0, 3 hours)
      - Impact: [description]
      - Actions: [concrete steps]

   2. **[Gap 2 Title]** (P1, 1.5 hours)
      - Impact: [description]
      - Actions: [concrete steps]

   Shall I create tasks and implement Gap 1?
   ```

5. **Offer to execute**: If user agrees, use TodoWrite to create tasks, then implement

### Report Structure

Generated reports include:
- **Current State**: Adoption level, completion %, next milestone
- **Validation Results**: Automated checks (âœ…/âŒ)
- **Gap Analysis**: Prioritized gaps (P0/P1/P2) with concrete actions
- **Sprint Plan**: This sprint focus, next sprint goals

Example gap:
```
### Gap 1: Test coverage 0% < 85% target (P0)
**Impact**: High - Blocks SAP-005 CI/CD coverage gates
**Effort**: Medium - 3 hours, 8 tests needed
**Actions**:
1. Generate coverage report (5 min)
2. Write 8 tests for API handlers (2.5 hours)
3. Validate â‰¥85% coverage (2 min)
```

### Integration with Sprint Planning

**Sprint Planning Workflow**:
1. Run strategic analysis to generate roadmap
2. Review priority gaps (top 5)
3. Select 1-2 gaps for current sprint
4. Add to sprint backlog with time estimates
5. Track progress via re-evaluation at sprint end

**Quarterly Review**:
1. Generate strategic roadmap
2. Compare to previous quarter
3. Calculate adoption velocity
4. Update quarterly goals

### Tips for AI Agents

- **Start with quick check**: Don't deep dive unless user asks
- **Focus on top 3 gaps**: Don't overwhelm with full list
- **Prioritize P0 gaps**: These block other SAPs
- **Be specific**: Reference exact commands, files, line numbers from reports
- **Track progress**: Re-run evaluation after implementing improvements

---

## Bidirectional Translation Layer (Mutual Ergonomics)

### Overview

**Purpose**: Enable natural conversational interaction while executing procedurally within the ecosystem ontology.

**Core Principle**: Meet users where they are. You adapt to their communication style; they don't need to memorize exact commands.

### Tools Available

#### 1. Intent Router (`scripts/intent-router.py`)

Translates natural language to formal actions with confidence scoring.

**Usage**:
```python
from intent_router import IntentRouter

router = IntentRouter("docs/dev-docs/patterns/INTENT_PATTERNS.yaml")
matches = router.route(user_input)

if matches and matches[0].confidence >= 0.7:
    # High confidence: execute
    action = matches[0].action
    execute_action(action, matches[0].parameters)
elif matches and matches[0].confidence >= 0.5:
    # Medium confidence: ask for clarification
    clarify_with_user(matches[0])
else:
    # Low confidence: show alternatives
    show_alternatives(matches[:3])
```

**Common Patterns**:
- "show inbox" â†’ `run_inbox_status`
- "how are saps" â†’ `run_sap_evaluator_quick`
- "i want to suggest a big change" â†’ `create_strategic_proposal`
- "review coordination requests" â†’ `review_coordination_requests`

**Pattern Learning**: When user says something repeatedly that maps successfully, add it to triggers.

#### 2. Glossary Search (`scripts/chora-search.py`)

Enables terminology discovery and reverse lookup.

**Usage**:
```bash
# Forward lookup: term â†’ definition
python scripts/chora-search.py "coordination request"

# Reverse lookup: description â†’ term
python scripts/chora-search.py --reverse "I want to suggest a big change"
# Returns: Strategic Proposal (95% confidence)

# Fuzzy matching: handles typos
python scripts/chora-search.py --fuzzy "coordenation"
```

**When to Use**:
- User asks "What is X?"
- User uses unfamiliar terminology
- User describes concept without using correct term
- User makes typos

#### 3. User Preferences (`.chora/user-preferences.yaml`)

Adapt behavior based on user working style.

**Key Preferences**:
- `verbosity`: concise | standard | verbose
- `formality`: casual | standard | formal
- `output_format`: terminal | markdown | json
- `require_confirmation`: always | destructive | never
- `progressive_disclosure`: true | false

**Loading**:
```python
import yaml
from pathlib import Path

prefs_file = Path(".chora/user-preferences.yaml")
if prefs_file.exists():
    with open(prefs_file) as f:
        prefs = yaml.safe_load(f)

    # Adapt verbosity
    if prefs["communication"]["verbosity"] == "concise":
        return brief_summary()
    elif prefs["communication"]["verbosity"] == "verbose":
        return detailed_report()
```

#### 4. Suggestion Engine (`scripts/suggest-next.py`)

Context-aware next action recommendations.

**Usage**:
```bash
# When user asks "what next?" or "what should I do?"
python scripts/suggest-next.py

# Proactive mode (high-priority suggestions only)
python scripts/suggest-next.py --mode proactive
```

**Context Signals**:
- Recent events (last 24 hours)
- Active work items (inbox/active/)
- Current phase (DDD/BDD/TDD detection)
- Quality metrics (coverage, tests, lint)
- Inbox backlog

### Communication Patterns

#### Pattern 1: Natural Input â†’ Execution â†’ Result

**Don't explain what you'll do, show what you did**:

```
âŒ Bad:
User: "Create coordination request for traceability"
Agent: "To create a coordination request, you should:
       1. Create JSON in inbox/incoming/coordination/
       2. Use schema from inbox/schemas/
       3. ..."

âœ… Good:
User: "Create coordination request for traceability"
Agent: âœ… Created coord-007-traceability-pilot.json
       âœ… Logged event: coordination_request_created
       Next: Submit for sprint planning (2025-11-15)?
```

#### Pattern 2: Progressive Formalization

Move from casual â†’ formal as needed:

```
"I want to add health monitoring" (casual)
  â†’ Strategic Proposal (semi-formal artifact)
  â†’ RFC (formal discussion)
  â†’ ADR (formal decision)
  â†’ Coordination Request (fully formal)
  â†’ Implementation (procedural)
```

#### Pattern 3: Adaptive Output

Match user preferences:

```python
# Concise user
"âœ… 12 SAPs installed. Top gap: coverage 60%"

# Verbose user
"""
SAP Adoption Status
===================
- 12/18 SAPs installed (67%)
- Level 2: SAP-004, SAP-019
- Top gap: Test coverage at 60% (need 85%)
  Action: Add 45 tests, est. 4-6 hours
"""
```

#### Pattern 4: Context Retention

Remember conversation context:

```
User: "Show inbox"
Agent: [Shows 3 coordination requests]

User: "Review the first one"
Agent: [Recognizes "first one" = coord-003]
       [Shows coord-003 details]
```

### Anti-Patterns (Avoid These)

âŒ **Forcing Exact Syntax**: "Command not recognized" when user says "show me the inbox"
âŒ **Explaining Instead of Executing**: Long explanations of what to do instead of doing it
âŒ **Ignoring Preferences**: Always verbose output even when user wants concise
âŒ **No Context**: Asking "what do you mean by 'it'?" when referent is clear
âŒ **Static Patterns**: Never learning new phrases user repeats successfully

### Learning & Adaptation

**Pattern Learning**:
- User says "what's cooking in the inbox?" 3+ times
- Intent router matches to `inbox_status` successfully
- Automatically add "what's cooking" to triggers
- Future queries: instant recognition

**Preference Evolution**:
- User says "too much detail" â†’ Update `verbosity: concise`
- User repeatedly cancels confirmations â†’ Suggest `require_confirmation: destructive`
- Track which features user uses â†’ Recommend relevant SAPs

**Expertise Tracking**:
- User has used coordination requests 10+ times
- Next time: Brief reminder instead of full explanation
- Adapt detail level as user gains expertise

### Integration Points

**See Also**:
- [Bidirectional Communication Pattern Guide](docs/dev-docs/workflows/BIDIRECTIONAL_COMMUNICATION.md) - Complete pattern documentation
- [Intent Patterns Database](docs/dev-docs/patterns/INTENT_PATTERNS.yaml) - All recognized patterns
- [Glossary](docs/GLOSSARY.md) - Ecosystem terminology (75+ terms)
- [User Preferences Template](.chora/user-preferences.yaml.template) - Configuration options

---

## Inbox Coordination (SAP-001 v1.1.0)

### Session Startup Routine

**Every new session, check inbox for coordination requests**:

```bash
# Visual status dashboard (recommended for human-readable overview)
python scripts/inbox-status.py

# Quick inbox count summary
python scripts/inbox-query.py --count-by-status

# View unacknowledged items (if any)
python scripts/inbox-query.py --incoming --unacknowledged --format summary
```

**Expected Output**:
```
Inbox Status Counts:
  Incoming Coordination: 12
  Incoming Tasks: 0
  Incoming Proposals: 0
  Active: 3
  Completed: 45
  Unacknowledged: 12
```

### Daily Inbox Workflows

#### Morning Routine (2-3 minutes)

1. **Check for new items**:
   ```bash
   python scripts/inbox-query.py --incoming --unacknowledged
   ```

2. **Review high-priority items** (P0, blocks_sprint urgency):
   ```bash
   # View specific request
   python scripts/inbox-query.py --request COORD-2025-XXX --format json
   ```

3. **Acknowledge within SLA** (1 business day, 4 hours for blocks_sprint):
   ```bash
   python scripts/respond-to-coordination.py \
     --request COORD-2025-XXX \
     --status acknowledged \
     --notes "Reviewing - will respond by [date]"
   ```

#### Processing Coordination Requests

**Accept and Start Work**:
```bash
python scripts/respond-to-coordination.py \
  --request COORD-2025-XXX \
  --status accepted \
  --effort "8-12 hours" \
  --timeline "Complete by 2025-11-15" \
  --notes "Starting implementation in current sprint" \
  --move-to-active
```

**Decline with Reason**:
```bash
python scripts/respond-to-coordination.py \
  --request COORD-2025-XXX \
  --status declined \
  --reason "Outside current scope. Recommend proposing for Q1 2026 roadmap."
```

**Check Active Work**:
```bash
python scripts/inbox-query.py --status in_progress
```

### Creating Coordination Requests

**Interactive Mode** (recommended for new users):
```bash
python scripts/generate-coordination-request.py --interactive
```

**Context File Mode** (for AI agents):
```bash
# Create context file
cat > inbox/draft/my-request-context.json << 'EOF'
{
  "title": "Add Feature X to Repository Y",
  "description": "Brief description of what needs to be done",
  "background": "Why this is needed, current state",
  "rationale": "Benefits and justification",
  "priority": "P1",
  "urgency": "next_sprint",
  "from_repo": "github.com/liminalcommons/chora-base",
  "to_repo": "github.com/liminalcommons/target-repo"
}
EOF

# Generate request with AI-powered deliverables and acceptance criteria
python scripts/generate-coordination-request.py \
  --context inbox/draft/my-request-context.json \
  --ai-model claude-sonnet-4-5-20250929 \
  --output inbox/draft/my-request.json
```

**Quality**: Generator produces 94.9% quality deliverables and SMART acceptance criteria

### Service Level Agreements (SLAs)

**Response Times by Urgency**:

| Urgency | Acknowledgment | Full Response | When to Use |
|---------|---------------|---------------|-------------|
| `blocks_sprint` | 4 hours | Same day | Blocking current sprint work |
| `next_sprint` | 1 business day | 3 business days | Needed for next sprint |
| `backlog` | 1 business day | 1 week | Non-urgent, future planning |

**SLA Compliance**:
- Check inbox at session startup
- Prioritize `blocks_sprint` items immediately
- Acknowledge all items within 1 business day
- Provide full response within urgency-based timeline

### Ecosystem Coordination

**Repository Status**: Active participant in ecosystem coordination

**Capabilities**:
- **Provides**: Template generation, SAP framework, coordination protocol
- **Receives**: Feature requests, bug reports, integration proposals

**Ecosystem Dashboard**: [inbox/ecosystem/ECOSYSTEM_STATUS.yaml](inbox/ecosystem/ECOSYSTEM_STATUS.yaml)

**Ecosystem Invitations**: See [inbox/ecosystem/invitations/](inbox/ecosystem/invitations/) for pending invitations

### Inbox Status Dashboard

**For generic agents and quick visual overview**:

```bash
# Full visual dashboard (recommended)
python scripts/inbox-status.py

# Detailed view with item listings
python scripts/inbox-status.py --detailed

# JSON export for programmatic access
python scripts/inbox-status.py --format json

# Markdown report for documentation
python scripts/inbox-status.py --format markdown > inbox-report.md

# Filter by priority
python scripts/inbox-status.py --priority P0 --detailed

# Recent activity (last 7 days)
python scripts/inbox-status.py --last 7d
```

**Dashboard Output Includes**:
- ğŸ“¥ Incoming queue counts (coordination, tasks, context)
- ğŸ”„ Active work items with details
- â±ï¸ Recent activity timeline (last 20 events)
- âœ… Recent completions (last 30 days)
- ğŸ’¡ Summary with actionable counts

**Use Cases**:
- Session startup: Get at-a-glance status
- Generic agent queries: "inbox status" â†’ run this tool
- Status reports: Export markdown for weekly updates
- Priority triage: Filter by P0/P1 for urgent items
- Trace investigation: `--trace-id` to follow request chains

### Troubleshooting

**Issue: "Item not found"**
- Use full filename as request ID: `COORD-2025-XXX-description` not just `COORD-2025-XXX`
- Check file exists: `ls inbox/incoming/coordination/COORD-2025-XXX*`

**Issue: "No items found"**
- Verify working directory: Must run commands from project root
- Check inbox path: `ls inbox/incoming/coordination/`

**Issue: Post-processing error**
- Skip post-processing: Remove `--post-process` flag
- Use custom output: `--output inbox/draft/my-request.json`

### Key Resources

- **Protocol Spec**: [docs/skilled-awareness/inbox/protocol-spec.md](docs/skilled-awareness/inbox/protocol-spec.md) (SAP-001 v1.1.0)
- **Onboarding Guide**: [docs/ECOSYSTEM_ONBOARDING.md](docs/ECOSYSTEM_ONBOARDING.md)
- **Phase 1 Summary**: [docs/PHASE_1_COMPLETION_SUMMARY.md](docs/PHASE_1_COMPLETION_SUMMARY.md)
- **Validation Report**: [docs/PHASE_1_VALIDATION_REPORT.md](docs/PHASE_1_VALIDATION_REPORT.md)
- **v1.1.0 Announcement**: [inbox/ecosystem/announcements/SAP-001-v1.1.0-RELEASE.md](inbox/ecosystem/announcements/SAP-001-v1.1.0-RELEASE.md)

---

## SAP Adoption Quarterly Review

### Overview

**When**: End of each quarter (Q1: March 31, Q2: June 30, Q3: September 30, Q4: December 31)

**Purpose**: Systematically evaluate SAP adoption progress, identify gaps, and plan next quarter's adoption roadmap using SAP-019 (self-evaluation) tools.

**Duration**: 30-60 minutes per quarter

**Outputs**:
- Strategic roadmap YAML (`project-docs/sap-roadmap-QX-YYYY.yaml`)
- Quarterly review markdown (`docs/adoption-reports/QX-YYYY-review.md`)
- Updated adoption history (`adoption-history.jsonl`)

### Process

**Step 1: Generate Strategic Roadmap** (5 minutes)

```bash
python scripts/sap-evaluator.py --strategic --output project-docs/sap-roadmap-Q{X}-{YEAR}.yaml
```

This analyzes all installed SAPs, identifies gaps, and creates a prioritized roadmap.

**Step 2: Create Review Document** (10 minutes)

```bash
python scripts/sap-evaluator.py --strategic --format markdown --output docs/adoption-reports/Q{X}-{YEAR}-review.md
```

Or manually create review document with:
- Progress since last quarter (SAPs installed, levels achieved)
- Velocity metrics (SAPs/month, level progressions/month)
- Successes and blockers
- Top 5 priority gaps from roadmap

**Step 3: Analyze Progress** (15 minutes)

Compare to previous quarter:
- How many SAPs progressed to higher levels?
- What was the adoption velocity?
- Which gaps were resolved?
- Which gaps remain unresolved (blockers)?

Calculate velocity:
- SAPs adopted per month
- Level progressions per month
- Hours invested vs hours saved (ROI)

**Step 4: Update Goals** (10 minutes)

Based on roadmap and analysis:
- Set targets for next quarter (# of SAPs to L2, # to L3)
- Prioritize 3-5 focus SAPs from roadmap
- Estimate ROI potential from priority gaps
- Identify blockers to address

**Step 5: Commit Review** (5 minutes)

```bash
git add project-docs/sap-roadmap-Q{X}-{YEAR}.yaml
git add docs/adoption-reports/Q{X}-{YEAR}-review.md
git add adoption-history.jsonl  # If manually updated
git commit -m "docs(sap-019): Q{X}-{YEAR} adoption review

- Strategic roadmap generated
- Progress analysis completed
- Next quarter goals set

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

### Example Review Document Structure

```markdown
# Q4-2025 SAP Adoption Review

**Generated**: 2025-12-31
**Review Period**: October 1 - December 31, 2025

## Progress Summary

### SAPs Adopted
- Total SAPs: 8 (was 6 in Q3)
- New adoptions: SAP-004, SAP-019

### Level Progression
- Level 2: 4 SAPs (was 2 in Q3)
- Level 3: 1 SAP (was 0 in Q3)
- Average level: 1.75 (was 1.33 in Q3)

### Velocity
- SAPs adopted: 0.67/month (2 in 3 months)
- Level progressions: 1.0/month (3 in 3 months)
- Hours invested: 12 hours this quarter

## Successes

1. **SAP-019 to L3**: Self-evaluation operational, strategic roadmaps generated
2. **SAP-004 to L2**: Test coverage improved from 0% to 75%
3. **SAP-013 integration**: ROI tracking now includes SAP adoption

## Blockers

1. **SAP-004 test coverage gap**: Still below 85% target, blocked by missing pytest fixtures
2. **SAP-009 documentation**: No domain-specific AGENTS.md files created yet

## Next Quarter Goals (Q1-2026)

### Targets
- Bring SAP-004 to L3 (complete test coverage to 85%+)
- Adopt 2 new SAPs (SAP-014, SAP-016)
- Achieve 2.0 average adoption level

### Focus SAPs
1. SAP-004 (testing-framework) - Priority P0, blocks sprint
2. SAP-009 (agent-awareness) - Priority P1, improves documentation
3. SAP-014 (mcp-server-development) - Priority P2, enables MCP development

### Estimated ROI
- Hours to invest: 8-10 hours
- Hours to save: 15-20 hours/quarter
- Expected ROI: 1.75x

## Roadmap

See [project-docs/sap-roadmap-Q1-2026.yaml](../project-docs/sap-roadmap-Q1-2026.yaml) for detailed sprint breakdown.
```

### Tips

**Automate Data Collection**:
- `adoption-history.jsonl` tracks all SAP events automatically
- `python scripts/sap-evaluator.py --quick` shows current state
- Compare git history: `git log --since="3 months ago" --grep="sap-"`

**Focus on Value**:
- Prioritize gaps with highest ROI (hours_saved / hours_invested)
- Address blockers first (gaps blocking other gaps)
- Celebrate wins (completed levels, new adoptions)

**Keep It Short**:
- 30-60 minutes maximum
- Use tools to generate data
- Focus on decisions, not documentation

---

## Task Tracking with Beads (SAP-015)

### Overview

SAP-015 provides persistent task tracking for AI agents using [beads](https://github.com/steveyegge/beads), a git-backed issue tracker designed for agent memory across sessions.

**When to use**:
- âœ… Complex features (5+ steps spanning multiple sessions)
- âœ… Tasks with dependencies (what blocks what)
- âœ… Multi-agent coordination
- âœ… Feature decomposition (epics â†’ subtasks)

**Don't use for**:
- âŒ Cross-repo coordination (use SAP-001 inbox)
- âŒ Event history (use SAP-010 A-MEM)
- âŒ Simple single-step tasks

### Quick Reference

**Session Start**:
```bash
bd ready --json                                    # Find unblocked work
bd update {id} --status in_progress --assignee {agent}  # Claim task
bd show {id} --json                                # Get task context
```

**During Work**:
```bash
bd create "Task title" --priority 0                # Add discovered subtask
bd dep add {blocked} {blocker}                     # Link dependency
bd update {parent} --status open                   # Parent now blocked
```

**Session End**:
```bash
bd close {id} --reason "Completed X"               # Mark done
bd ready --json                                    # Check newly-unblocked work
```

**Status Check**:
```bash
bd list --status in_progress --json                # Active tasks
bd dep tree {id}                                   # Visualize dependencies
bd status                                          # Database overview
```

### Integration with Other SAPs

**SAP-001 (Inbox)**:
- Decompose coordination requests into beads tasks
- Create epic for coordination, link subtasks
- See [Pattern H](docs/skilled-awareness/task-tracking/awareness-guide.md#pattern-h-integration-with-inbox-sap-001)

**SAP-010 (A-MEM)**:
- Correlate beads tasks with event traces
- Include trace_id in task description
- Log task events in A-MEM event log
- See [Pattern I](docs/skilled-awareness/task-tracking/awareness-guide.md#pattern-i-integration-with-a-mem-sap-010)

### Full Documentation

- **Adoption Blueprint**: [docs/skilled-awareness/task-tracking/adoption-blueprint.md](docs/skilled-awareness/task-tracking/adoption-blueprint.md)
- **Awareness Guide**: [docs/skilled-awareness/task-tracking/awareness-guide.md](docs/skilled-awareness/task-tracking/awareness-guide.md)
- **Protocol Spec**: [docs/skilled-awareness/task-tracking/protocol-spec.md](docs/skilled-awareness/task-tracking/protocol-spec.md)
- **Capability Charter**: [docs/skilled-awareness/task-tracking/capability-charter.md](docs/skilled-awareness/task-tracking/capability-charter.md)

---

## Questions & Support

### Where to Find Information

**Template configuration**: `docs/reference/template-configuration.md`
**Upgrade philosophy**: `docs/upgrades/PHILOSOPHY.md`
**Version history**: `CHANGELOG.md`
**ROI analysis**: `docs/BENEFITS.md`
**Research**: `docs/research/`

### Where to Report Issues

**GitHub Issues**: https://github.com/liminalcommons/chora-base/issues
**Discussions**: https://github.com/liminalcommons/chora-base/discussions

### Version Information

**Current Version**: v1.9.3 (2025-10-22)
**Next Version**: v2.0.0 (MAJOR - nested AGENTS.md refactoring)

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-10-22 | Initial AGENTS.md for chora-base repository (Phase 2 of v1.9.3 release) |

---

**End of AGENTS.md**

This document is the **source of truth** for AI agents and human developers working on chora-base. When in doubt, refer here first.
