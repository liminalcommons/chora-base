---
title: Process Metrics & KPIs
category: project-management
audience: human-developers, ai-agents, stakeholders
lifecycle_phase: all-phases
created: 2025-10-25
updated: 2025-10-25
---

# Process Metrics & KPIs

**Purpose**: Track development process health, quality, and velocity using evidence-based metrics.

**Audience**: Human developers, AI agents, engineering leads, stakeholders

**Related Workflows**:
- [8-Phase Development Process](../../dev-docs/workflows/DEVELOPMENT_PROCESS.md) - All phases
- [Development Lifecycle](../../dev-docs/workflows/DEVELOPMENT_LIFECYCLE.md)
- [Sprint Planning](../sprints/README.md)
- [Release Planning](../releases/RELEASE_PLANNING_GUIDE.md)

---

## Quick Start

### For AI Agents: Metrics Collection Decision Tree

```
START: Metrics collection needed
    ‚Üì
Q1: What are you measuring?
    QUALITY ‚Üí Defects, coverage, technical debt
    VELOCITY ‚Üí Cycle time, throughput, burndown
    PROCESS ‚Üí Adherence to DDD/BDD/TDD workflows
    ADOPTION ‚Üí User downloads, upgrade rate, satisfaction
    ‚Üì
Q2: What frequency?
    DAILY ‚Üí Velocity, defects found
    WEEKLY ‚Üí Sprint progress, quality gates
    MONTHLY ‚Üí Trends, process improvements
    PER RELEASE ‚Üí Adoption, satisfaction
    ‚Üì
Q3: Where to record?
    SPRINT METRICS ‚Üí project-docs/sprints/sprint-N.md
    RELEASE METRICS ‚Üí project-docs/releases/release-vX.Y.Z.md
    PROCESS TRENDS ‚Üí THIS FILE (PROCESS_METRICS.md)
    ‚Üì
EXECUTE: Update metrics
    1. Collect data (automated or manual)
    2. Calculate KPIs
    3. Update appropriate document
    4. Analyze trends (compare to targets)
    5. Identify improvements (if off-target)
    ‚Üì
OUTCOME: Data-driven process improvements
```

---

## Overview

### Why Measure?

**Benefits**:
1. **Data-driven decisions** - Replace gut feeling with evidence
2. **Early warning system** - Detect problems before they escalate
3. **Continuous improvement** - Track impact of process changes
4. **Stakeholder communication** - Demonstrate progress and quality

**What NOT to Measure**:
- ‚ùå Lines of code (encourages verbosity)
- ‚ùå Hours worked (encourages presenteeism)
- ‚ùå Commits per day (encourages micro-commits)
- ‚ùå Individual productivity (encourages gaming metrics)

**What TO Measure**:
- ‚úÖ Defect rate (quality indicator)
- ‚úÖ Test coverage (safety net indicator)
- ‚úÖ Cycle time (efficiency indicator)
- ‚úÖ Sprint velocity (predictability indicator)
- ‚úÖ Process adherence (best practices indicator)

---

## Metric Categories

### 1. Quality Metrics

**Purpose**: Measure software quality and technical debt

#### Defect Rate

**Definition**: Number of bugs per release or sprint

**Calculation**:
```
Defect Rate = Total Defects / Release (or Sprint)
```

**Targets**:
- ‚úÖ Green: <3 defects per release
- ‚ö†Ô∏è Yellow: 3-7 defects per release
- üî¥ Red: >7 defects per release

**Evidence**: TDD projects see 40-80% fewer defects (Microsoft Research)

**Example Tracking**:
```markdown
| Release | Total Defects | Critical (P0) | High (P1) | Medium (P2) | Status |
|---------|---------------|---------------|-----------|-------------|--------|
| v1.4.0  | 2             | 0             | 1         | 1           | ‚úÖ     |
| v1.5.0  | 5             | 0             | 2         | 3           | ‚ö†Ô∏è     |
| v1.6.0  | 1             | 0             | 0         | 1           | ‚úÖ     |

**Trend**: Improving (2 ‚Üí 5 ‚Üí 1)
**Action**: Continue TDD/BDD practices
```

---

#### Test Coverage

**Definition**: Percentage of code covered by automated tests

**Calculation**:
```
Test Coverage = (Lines Covered / Total Lines) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•90% coverage
- ‚ö†Ô∏è Yellow: 80-89% coverage
- üî¥ Red: <80% coverage

**Evidence**: >90% coverage correlates with 40-80% fewer production defects

**Example Tracking**:
```markdown
| Sprint | Overall | Unit | Integration | E2E | Status |
|--------|---------|------|-------------|-----|--------|
| Sprint 5 | 89% | 92% | 85% | N/A | ‚ö†Ô∏è |
| Sprint 6 | 93% | 95% | 90% | N/A | ‚úÖ |
| Sprint 7 | 94% | 96% | 91% | 3 scenarios | ‚úÖ |

**Trend**: Improving (89% ‚Üí 93% ‚Üí 94%)
**Action**: Maintain DDD/BDD/TDD workflow adherence
```

---

#### Technical Debt

**Definition**: Estimated time to address code quality issues

**Calculation**:
```
Technical Debt Ratio = (Remediation Time / Development Time) √ó 100%
```

**Targets**:
- ‚úÖ Green: <5% technical debt ratio
- ‚ö†Ô∏è Yellow: 5-10% technical debt ratio
- üî¥ Red: >10% technical debt ratio

**Example Tracking**:
```markdown
| Quarter | Tech Debt (hours) | Dev Time (hours) | Ratio | Status |
|---------|-------------------|------------------|-------|--------|
| Q1 2025 | 40                | 480              | 8.3%  | ‚ö†Ô∏è     |
| Q2 2025 | 24                | 520              | 4.6%  | ‚úÖ     |

**Trend**: Improving (8.3% ‚Üí 4.6%)
**Action**: Allocate 20% of sprint capacity to refactoring
```

---

### 2. Velocity Metrics

**Purpose**: Measure development speed and predictability

#### Sprint Velocity

**Definition**: Percentage of committed work completed per sprint

**Calculation**:
```
Sprint Velocity = (Story Points Completed / Story Points Committed) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•80% velocity (healthy capacity planning)
- ‚ö†Ô∏è Yellow: 60-79% velocity (over-commitment)
- üî¥ Red: <60% velocity (serious planning issues)

**Example Tracking**:
```markdown
| Sprint | Committed | Completed | Velocity | Status |
|--------|-----------|-----------|----------|--------|
| Sprint 3 | 21 | 17 | 81% | ‚úÖ |
| Sprint 4 | 24 | 22 | 92% | ‚úÖ |
| Sprint 5 | 21 | 18 | 86% | ‚úÖ |
| **3-Sprint Avg** | **22** | **19** | **86%** | **‚úÖ** |

**Trend**: Stable and predictable
**Action**: Maintain current capacity planning approach
```

---

#### Cycle Time

**Definition**: Time from story start to production deployment

**Calculation**:
```
Cycle Time = Deployment Date - Start Date
```

**Targets**:
- ‚úÖ Green: <3 days per story
- ‚ö†Ô∏è Yellow: 3-5 days per story
- üî¥ Red: >5 days per story

**Evidence**: DDD/BDD/TDD reduces cycle time by 30-40% (no rework)

**Example Tracking**:
```markdown
| Story | Start Date | Deploy Date | Cycle Time | Status |
|-------|------------|-------------|------------|--------|
| OAuth2 Login | 2025-02-01 | 2025-02-03 | 2 days | ‚úÖ |
| Analytics Dashboard | 2025-02-04 | 2025-02-07 | 3 days | ‚úÖ |
| Export CSV | 2025-02-08 | 2025-02-15 | 7 days | üî¥ |

**Trend**: Export CSV took longer (complex feature, external dependency)
**Action**: Break down large stories, identify dependencies early
```

---

#### Lead Time

**Definition**: Time from backlog to production

**Calculation**:
```
Lead Time = Deployment Date - Backlog Date
```

**Targets**:
- ‚úÖ Green: <2 weeks
- ‚ö†Ô∏è Yellow: 2-4 weeks
- üî¥ Red: >4 weeks

**Example Tracking**:
```markdown
| Feature | Backlog Date | Deploy Date | Lead Time | Status |
|---------|--------------|-------------|-----------|--------|
| User Auth | 2025-01-15 | 2025-02-05 | 21 days | ‚ö†Ô∏è |
| Analytics | 2025-01-20 | 2025-02-10 | 21 days | ‚ö†Ô∏è |

**Trend**: Consistent at 3 weeks
**Action**: Acceptable for 2-week sprint cycle (includes planning + development)
```

---

### 3. Process Adherence Metrics

**Purpose**: Measure adoption of DDD/BDD/TDD workflows

#### DDD Adherence

**Definition**: Percentage of features with documentation written first

**Calculation**:
```
DDD Adherence = (Features with Docs First / Total Features) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•90% adherence
- ‚ö†Ô∏è Yellow: 70-89% adherence
- üî¥ Red: <70% adherence

**Evidence**: DDD reduces rework by 40-60% (saves 8-15 hours per feature)

**Example Tracking**:
```markdown
| Sprint | Total Features | DDD First | Adherence | Rework Hours | Status |
|--------|----------------|-----------|-----------|--------------|--------|
| Sprint 3 | 5 | 3 | 60% | 12 | üî¥ |
| Sprint 4 | 4 | 4 | 100% | 0 | ‚úÖ |
| Sprint 5 | 5 | 5 | 100% | 0 | ‚úÖ |

**Trend**: Improved from 60% to 100%
**ROI**: Saved 24 hours of rework in Sprints 4-5
```

---

#### BDD Adherence

**Definition**: Percentage of features with Gherkin scenarios before implementation

**Calculation**:
```
BDD Adherence = (Features with BDD First / Total Features) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•80% adherence
- ‚ö†Ô∏è Yellow: 60-79% adherence
- üî¥ Red: <60% adherence

**Example Tracking**:
```markdown
| Sprint | Total Features | BDD First | Adherence | Acceptance Issues | Status |
|--------|----------------|-----------|-----------|-------------------|--------|
| Sprint 3 | 5 | 2 | 40% | 3 | üî¥ |
| Sprint 4 | 4 | 3 | 75% | 1 | ‚ö†Ô∏è |
| Sprint 5 | 5 | 5 | 100% | 0 | ‚úÖ |

**Trend**: Improving (40% ‚Üí 75% ‚Üí 100%)
**Impact**: Zero acceptance issues when BDD used first
```

---

#### TDD Adherence

**Definition**: Percentage of code written with test-first approach

**Calculation**:
```
TDD Adherence = (Functions with Tests First / Total Functions) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•80% adherence
- ‚ö†Ô∏è Yellow: 60-79% adherence
- üî¥ Red: <60% adherence

**Evidence**: TDD reduces defect rate by 40-80%

**Example Tracking**:
```markdown
| Sprint | Total Functions | TDD First | Adherence | Defects Found | Status |
|--------|----------------|-----------|-----------|---------------|--------|
| Sprint 3 | 25 | 15 | 60% | 5 | üî¥ |
| Sprint 4 | 20 | 18 | 90% | 1 | ‚úÖ |
| Sprint 5 | 22 | 20 | 91% | 0 | ‚úÖ |

**Trend**: Improving (60% ‚Üí 90% ‚Üí 91%)
**Correlation**: Defect rate dropped from 5 to 0
```

---

### 4. Adoption Metrics (Post-Release)

**Purpose**: Measure user adoption and satisfaction

#### Download/Install Rate

**Definition**: Number of downloads or installs per release

**Targets**:
- ‚úÖ Green: Growth ‚â•10% over previous release
- ‚ö†Ô∏è Yellow: Growth 0-9%
- üî¥ Red: Decline

**Example Tracking**:
```markdown
| Release | PyPI Downloads (7 days) | Docker Pulls (7 days) | Growth | Status |
|---------|-------------------------|----------------------|--------|--------|
| v1.4.0  | 450                     | 120                  | -      | -      |
| v1.5.0  | 520                     | 135                  | +15%   | ‚úÖ     |
| v1.6.0  | 650                     | 160                  | +25%   | ‚úÖ     |

**Trend**: Strong growth (15% ‚Üí 25%)
**Driver**: OAuth2 feature highly requested
```

---

#### Upgrade Rate

**Definition**: Percentage of users who upgraded to latest version

**Calculation**:
```
Upgrade Rate = (Users on Latest / Total Active Users) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•60% upgrade rate (1 week post-release)
- ‚ö†Ô∏è Yellow: 40-59% upgrade rate
- üî¥ Red: <40% upgrade rate

**Example Tracking**:
```markdown
| Release | Total Users | Upgraded (1 week) | Upgrade Rate | Status |
|---------|-------------|-------------------|--------------|--------|
| v1.5.0  | 1,200       | 720               | 60%          | ‚úÖ     |
| v1.6.0  | 1,350       | 975               | 72%          | ‚úÖ     |

**Trend**: Improving (60% ‚Üí 72%)
**Driver**: Clear upgrade guide, backward compatible
```

---

#### User Satisfaction

**Definition**: Percentage of positive feedback vs. total feedback

**Calculation**:
```
User Satisfaction = (Positive Feedback / Total Feedback) √ó 100%
```

**Targets**:
- ‚úÖ Green: ‚â•85% satisfaction
- ‚ö†Ô∏è Yellow: 70-84% satisfaction
- üî¥ Red: <70% satisfaction

**Example Tracking**:
```markdown
| Release | Positive | Negative | Neutral | Total | Satisfaction | Status |
|---------|----------|----------|---------|-------|--------------|--------|
| v1.5.0  | 42       | 5        | 8       | 55    | 76%          | ‚ö†Ô∏è     |
| v1.6.0  | 58       | 3        | 4       | 65    | 89%          | ‚úÖ     |

**Trend**: Improving (76% ‚Üí 89%)
**Driver**: Performance improvements, smooth upgrade experience
```

---

## Metrics Dashboard Template

### Sprint Metrics Dashboard

**Location**: Update in `project-docs/sprints/sprint-N.md`

```markdown
## Sprint N Metrics (Updated 2025-MM-DD)

### Quality Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **Test Coverage** | ‚â•90% | 93% | ‚úÖ |
| **Defects Found** | <3 | 1 | ‚úÖ |
| **Tech Debt Added** | <2 hours | 0 hours | ‚úÖ |

### Velocity Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **Story Points Completed** | 21 | 18 | 86% ‚úÖ |
| **Cycle Time (avg)** | <3 days | 2.5 days | ‚úÖ |
| **Blocked Days** | <2 days | 0 days | ‚úÖ |

### Process Adherence

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **DDD Adherence** | ‚â•90% | 100% | ‚úÖ |
| **BDD Adherence** | ‚â•80% | 100% | ‚úÖ |
| **TDD Adherence** | ‚â•80% | 91% | ‚úÖ |

**Overall Sprint Health**: ‚úÖ Excellent
```

---

### Release Metrics Dashboard

**Location**: Update in `project-docs/releases/release-vX.Y.Z.md`

```markdown
## Release vX.Y.Z Metrics (1 Week Post-Release)

### Quality Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Pre-Release Test Coverage** | ‚â•90% | 94% | ‚úÖ |
| **Defects (Production)** | <3 | 1 | ‚úÖ |
| **Performance vs. Baseline** | Within 10% | +5% faster | ‚úÖ |
| **Security Vulnerabilities** | 0 critical | 0 | ‚úÖ |

### Adoption Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Downloads (PyPI, 7 days)** | 500 | 650 | ‚úÖ (+30%) |
| **Upgrade Rate (7 days)** | ‚â•60% | 72% | ‚úÖ |
| **User Satisfaction** | ‚â•85% | 89% | ‚úÖ |
| **Support Tickets** | <10 | 4 | ‚úÖ |

### Deployment Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Deployment Time** | <30 min | 25 min | ‚úÖ |
| **Downtime** | 0 min | 0 min | ‚úÖ |
| **Rollback Required** | No | No | ‚úÖ |

**Overall Release Rating**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)
```

---

### Process Trends Dashboard

**Location**: Update in THIS FILE (`PROCESS_METRICS.md`)

```markdown
## Process Trends (Updated 2025-MM-DD)

### Quality Trend (Last 6 Sprints)

| Sprint | Test Coverage | Defects | Tech Debt | Overall |
|--------|---------------|---------|-----------|---------|
| Sprint 1 | 85% | 5 | 8.5% | ‚ö†Ô∏è |
| Sprint 2 | 88% | 3 | 6.2% | ‚ö†Ô∏è |
| Sprint 3 | 89% | 2 | 5.1% | ‚úÖ |
| Sprint 4 | 92% | 1 | 4.8% | ‚úÖ |
| Sprint 5 | 93% | 1 | 4.6% | ‚úÖ |
| Sprint 6 | 94% | 0 | 3.9% | ‚úÖ |

**Trend**: Improving consistently across all quality metrics

### Velocity Trend (Last 6 Sprints)

| Sprint | Velocity | Cycle Time (avg) | Lead Time (avg) | Overall |
|--------|----------|------------------|-----------------|---------|
| Sprint 1 | 75% | 4.2 days | 25 days | ‚ö†Ô∏è |
| Sprint 2 | 82% | 3.8 days | 23 days | ‚úÖ |
| Sprint 3 | 81% | 3.5 days | 21 days | ‚úÖ |
| Sprint 4 | 92% | 2.8 days | 19 days | ‚úÖ |
| Sprint 5 | 86% | 2.5 days | 18 days | ‚úÖ |
| Sprint 6 | 89% | 2.3 days | 17 days | ‚úÖ |

**Trend**: Velocity stabilizing at 86-92%, cycle time decreasing

### Process Adherence Trend (Last 6 Sprints)

| Sprint | DDD | BDD | TDD | ROI (hours saved) | Overall |
|--------|-----|-----|-----|-------------------|---------|
| Sprint 1 | 40% | 20% | 50% | 0 (baseline) | üî¥ |
| Sprint 2 | 50% | 40% | 60% | 4 | üî¥ |
| Sprint 3 | 60% | 40% | 60% | 8 | üî¥ |
| Sprint 4 | 100% | 75% | 90% | 18 | ‚úÖ |
| Sprint 5 | 100% | 100% | 91% | 24 | ‚úÖ |
| Sprint 6 | 100% | 100% | 95% | 26 | ‚úÖ |

**Trend**: Adherence improving dramatically, ROI increasing
**Impact**: Saved 78 hours total in Sprints 2-6
```

---

## How to Use This File

### For Daily Updates

**Velocity Metrics** (updated daily during sprint):
1. Open current sprint document (`project-docs/sprints/sprint-N.md`)
2. Update "Sprint Metrics" section
3. Review against targets
4. Flag any red/yellow metrics for attention

### For Weekly Updates

**Process Adherence** (updated weekly, end of sprint):
1. Calculate DDD/BDD/TDD adherence for sprint
2. Update sprint document
3. Update process trends in THIS FILE
4. Analyze trends (improving/stable/declining)

### For Release Updates

**Adoption Metrics** (updated 1 week post-release):
1. Collect download/upgrade data (PyPI, Docker, GitHub)
2. Gather user feedback (support tickets, issues, social media)
3. Update release document (`project-docs/releases/release-vX.Y.Z.md`)
4. Calculate satisfaction score

### For Quarterly Reviews

**Process Trends** (updated quarterly):
1. Review last 12 weeks (6 sprints) of data
2. Identify patterns (improvements, regressions)
3. Calculate ROI of process changes
4. Update THIS FILE with trends
5. Create action items for next quarter

---

## Metric Collection Automation

### Automated Metrics (via CI/CD)

**Test Coverage**:
```yaml
# .github/workflows/test.yml
- name: Generate coverage report
  run: pytest --cov=src --cov-report=json

- name: Extract coverage percentage
  run: |
    COVERAGE=$(jq '.totals.percent_covered' coverage.json)
    echo "Test coverage: $COVERAGE%"
```

**Defect Rate**:
```yaml
# .github/workflows/issue-metrics.yml
- name: Count bugs in milestone
  run: |
    BUGS=$(gh issue list --milestone "v1.6.0" --label bug --json number | jq 'length')
    echo "Defects in release: $BUGS"
```

### Manual Metrics

**Process Adherence**:
```markdown
At end of sprint, manually review:
1. How many features had DDD docs first? (check git history)
2. How many had BDD scenarios before implementation? (check PR order)
3. How many functions used TDD? (check test commits vs. code commits)

Record in sprint document.
```

**User Satisfaction**:
```markdown
At 1 week post-release, manually review:
1. GitHub issues (label: feedback)
2. Support tickets
3. Social media mentions
4. Email feedback

Categorize as Positive/Negative/Neutral, record in release document.
```

---

## ROI Analysis

### Time Savings from DDD/BDD/TDD

**Evidence-Based Estimates**:

| Metric | Before DDD/BDD/TDD | After DDD/BDD/TDD | Improvement |
|--------|-------------------|-------------------|-------------|
| **Defect Rate** | 15 bugs/release | 3 bugs/release | 80% reduction |
| **Test Coverage** | 65% | 92% | +27% |
| **Rework Time** | 35% of dev time | 10% of dev time | 71% reduction |
| **Cycle Time** | 5 days/story | 2.5 days/story | 50% reduction |
| **Release Confidence** | 60% (frequent rollbacks) | 95% (rare rollbacks) | +35% |

**ROI Calculation** (per sprint):

```
Sprint Duration: 2 weeks (80 hours committed)

BEFORE DDD/BDD/TDD:
- Rework: 35% √ó 80 hours = 28 hours
- Bug fixes (post-release): 10 hours
- Total waste: 38 hours

AFTER DDD/BDD/TDD:
- Rework: 10% √ó 80 hours = 8 hours
- Bug fixes (post-release): 2 hours
- Total waste: 10 hours

TIME SAVED: 38 - 10 = 28 hours per sprint
            = 14 hours per week
            = 728 hours per year

AT $150/hour: $109,200 saved per year (per developer)
```

---

## Anti-Patterns

### ‚ùå Anti-Pattern: Measuring Individual Productivity

**Problem**:
```
Metrics Dashboard:
- Alice: 25 commits, 500 LOC
- Bob: 18 commits, 350 LOC

Conclusion: Alice is more productive
```

**Impact**:
- Gaming metrics (micro-commits, verbose code)
- Competition instead of collaboration
- Focus on quantity, not quality

**Solution**:
```markdown
‚úÖ GOOD: Measure Team Outcomes

Metrics Dashboard:
- Sprint Velocity: 86% (team)
- Defect Rate: 1 bug (team)
- Test Coverage: 94% (team)

Conclusion: Team is healthy and productive
```

---

### ‚ùå Anti-Pattern: Ignoring Red Metrics

**Problem**:
```
Sprint 5 Metrics:
- Test Coverage: 75% üî¥ (target: ‚â•90%)
- Defects: 7 üî¥ (target: <3)

Action: None (shipped anyway)
```

**Impact**:
- Quality degradation
- Production issues
- Loss of user trust

**Solution**:
```markdown
‚úÖ GOOD: Address Red Metrics Before Shipping

Sprint 5 Metrics:
- Test Coverage: 75% üî¥

Action Taken:
1. Feature freeze (no new work)
2. Add missing tests (2 days)
3. Coverage improved to 91% ‚úÖ
4. Then shipped to production
```

---

### ‚ùå Anti-Pattern: Vanity Metrics

**Problem**:
```
Metrics Dashboard:
- Total commits: 450
- Total LOC: 15,000
- Files changed: 120

Conclusion: Lots of activity!
```

**Impact**:
- No insight into quality or value
- Encourages busy work
- Distracts from meaningful metrics

**Solution**:
```markdown
‚úÖ GOOD: Actionable Metrics

Metrics Dashboard:
- Defect Rate: 1 bug (quality indicator) ‚úÖ
- Test Coverage: 94% (safety net indicator) ‚úÖ
- Sprint Velocity: 86% (predictability indicator) ‚úÖ
- User Satisfaction: 89% (value indicator) ‚úÖ

Conclusion: High quality, predictable delivery, happy users
```

---

## For AI Agents: Metrics Update Checklist

### Daily (During Sprint)

- [ ] Update sprint burndown (story points remaining)
- [ ] Update cycle time for completed stories
- [ ] Flag any blocked work (>1 day)
- [ ] Update test coverage if dropping below target

### Weekly (End of Sprint)

- [ ] Calculate sprint velocity (completed/committed)
- [ ] Calculate DDD/BDD/TDD adherence
- [ ] Count defects found during sprint
- [ ] Update sprint metrics in sprint document
- [ ] Update process trends in PROCESS_METRICS.md

### Per Release

- [ ] Collect pre-release quality metrics (coverage, defects)
- [ ] Track deployment metrics (time, downtime, rollback)
- [ ] Collect 1-week post-release adoption metrics
- [ ] Calculate user satisfaction
- [ ] Update release metrics in release document
- [ ] Update release trend in PROCESS_METRICS.md

### Quarterly

- [ ] Review 12-week trends (quality, velocity, adherence)
- [ ] Calculate ROI of process improvements
- [ ] Identify patterns (what's improving, what's not)
- [ ] Create action items for next quarter
- [ ] Update PROCESS_METRICS.md with quarterly summary

---

## References

**Related Documentation**:
- [8-Phase Development Process](../../dev-docs/workflows/DEVELOPMENT_PROCESS.md) - Process overview
- [Development Lifecycle](../../dev-docs/workflows/DEVELOPMENT_LIFECYCLE.md) - Integration guide
- [Sprint Planning](../sprints/README.md) - Sprint velocity and burndown
- [Release Planning](../releases/RELEASE_PLANNING_GUIDE.md) - Release metrics and adoption

**Research**:
- [Agentic Coding Best Practices](../../docs/research/Agentic Coding Best Practices Research.pdf) - Evidence for metrics
- Microsoft Research: TDD reduces defect rate by 40-80%
- Google: >90% test coverage correlates with fewer production defects

---

**Document Version**: 1.0
**Last Updated**: 2025-10-25
**Maintained By**: chora-base v3.0.0
**License**: MIT

---

## Current Process Metrics

> **Note**: Update this section regularly (weekly for sprints, per release, quarterly for trends)

### Last Updated: YYYY-MM-DD

**Status**: ‚úÖ Green | ‚ö†Ô∏è Yellow | üî¥ Red

[Metrics data will be populated here as sprints and releases progress]
